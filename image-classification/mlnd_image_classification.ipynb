{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222bf98cf60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return np.divide(x, 255.)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    a = np.zeros((len(x), 10), dtype=np.float32)\n",
    "    for i, j in enumerate(x):\n",
    "        a[i,j]  = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, image_shape[0], image_shape[1], \n",
    "                                             image_shape[2]), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], \n",
    "                                               conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.constant(1.0, shape=[conv_num_outputs]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, weights, [1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + bias)\n",
    "    conv = tf.nn.max_pool(conv, [1, pool_strides[0], pool_strides[0], 1], \n",
    "                          [1, pool_ksize[0], pool_ksize[1], 1], padding='SAME')\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    new_dim = 1\n",
    "    for i in shape[1:]:\n",
    "        new_dim *= i\n",
    "    x = tf.reshape(x_tensor, (-1, new_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal([shape[1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.constant(1.0, shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor, weights)\n",
    "    layer = tf.add(layer, bias)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal([shape[1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.constant(1.0, shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor, weights)\n",
    "    layer = tf.add(layer, bias)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    model = conv2d_maxpool(x, conv_num_outputs=8, conv_ksize=(10,10), \n",
    "                           conv_strides=(1,1), pool_ksize=(2,2), \n",
    "                           pool_strides=(2,2))    \n",
    "    model = conv2d_maxpool(model, conv_num_outputs=16, conv_ksize=(5,5), \n",
    "                           conv_strides=(1,1), pool_ksize=(2,2), \n",
    "                           pool_strides=(2,2))    \n",
    "    model = conv2d_maxpool(model, conv_num_outputs=32, conv_ksize=(3,3), \n",
    "                           conv_strides=(1,1), pool_ksize=(2,2), \n",
    "                           pool_strides=(1,1))    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    model = flatten(model)    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    model = fully_conn(model, 1024)\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "    model = fully_conn(model, 256)\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "    model = fully_conn(model, 128)\n",
    "    model = tf.nn.dropout(model, keep_prob)    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    model = output(model, 10)    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    feed_dict = {x:feature_batch, y:label_batch, keep_prob:keep_probability}\n",
    "    fetches = [optimizer]\n",
    "    session.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    feed_dict = {x:feature_batch, y:label_batch, keep_prob:keep_probability}\n",
    "    loss, acc = session.run([cost, accuracy], feed_dict=feed_dict)\n",
    "    print('\\tBatch: loss {:6.4f} and accuracy {:4.2f}%'.format(loss, acc*100), end='\\t\\t')\n",
    "    \n",
    "    feed_dict = {x:valid_features, y:valid_labels, keep_prob:1}\n",
    "    loss, acc = session.run([cost, accuracy], feed_dict=feed_dict)\n",
    "    print('Valid: loss {:6.4f}, accuracy {:4.2f}%'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \tBatch: loss 5.2844 and accuracy 7.80%\t\tValid: loss 2.3669, accuracy 8.34%\n",
      "Epoch  2, CIFAR-10 Batch 1:  \tBatch: loss 3.1522 and accuracy 10.27%\t\tValid: loss 2.3258, accuracy 9.54%\n",
      "Epoch  3, CIFAR-10 Batch 1:  \tBatch: loss 2.7748 and accuracy 11.01%\t\tValid: loss 2.3193, accuracy 8.66%\n",
      "Epoch  4, CIFAR-10 Batch 1:  \tBatch: loss 2.5735 and accuracy 10.89%\t\tValid: loss 2.2909, accuracy 8.48%\n",
      "Epoch  5, CIFAR-10 Batch 1:  \tBatch: loss 2.5797 and accuracy 10.52%\t\tValid: loss 2.2893, accuracy 11.04%\n",
      "Epoch  6, CIFAR-10 Batch 1:  \tBatch: loss 2.5225 and accuracy 11.26%\t\tValid: loss 2.2877, accuracy 13.00%\n",
      "Epoch  7, CIFAR-10 Batch 1:  \tBatch: loss 2.4955 and accuracy 9.90%\t\tValid: loss 2.2876, accuracy 15.78%\n",
      "Epoch  8, CIFAR-10 Batch 1:  \tBatch: loss 2.4434 and accuracy 9.03%\t\tValid: loss 2.2901, accuracy 14.72%\n",
      "Epoch  9, CIFAR-10 Batch 1:  \tBatch: loss 2.4176 and accuracy 10.89%\t\tValid: loss 2.2837, accuracy 14.28%\n",
      "Epoch 10, CIFAR-10 Batch 1:  \tBatch: loss 2.3641 and accuracy 12.75%\t\tValid: loss 2.2861, accuracy 15.50%\n",
      "Epoch 11, CIFAR-10 Batch 1:  \tBatch: loss 2.3478 and accuracy 12.00%\t\tValid: loss 2.2868, accuracy 15.06%\n",
      "Epoch 12, CIFAR-10 Batch 1:  \tBatch: loss 2.3260 and accuracy 12.25%\t\tValid: loss 2.2782, accuracy 15.42%\n",
      "Epoch 13, CIFAR-10 Batch 1:  \tBatch: loss 2.3161 and accuracy 15.35%\t\tValid: loss 2.2677, accuracy 15.68%\n",
      "Epoch 14, CIFAR-10 Batch 1:  \tBatch: loss 2.2609 and accuracy 16.83%\t\tValid: loss 2.2439, accuracy 19.96%\n",
      "Epoch 15, CIFAR-10 Batch 1:  \tBatch: loss 2.2375 and accuracy 15.59%\t\tValid: loss 2.2163, accuracy 20.22%\n",
      "Epoch 16, CIFAR-10 Batch 1:  \tBatch: loss 2.2382 and accuracy 18.44%\t\tValid: loss 2.1818, accuracy 22.06%\n",
      "Epoch 17, CIFAR-10 Batch 1:  \tBatch: loss 2.2016 and accuracy 19.31%\t\tValid: loss 2.1578, accuracy 21.78%\n",
      "Epoch 18, CIFAR-10 Batch 1:  \tBatch: loss 2.2226 and accuracy 18.81%\t\tValid: loss 2.0981, accuracy 23.06%\n",
      "Epoch 19, CIFAR-10 Batch 1:  \tBatch: loss 2.1837 and accuracy 18.07%\t\tValid: loss 2.0830, accuracy 26.42%\n",
      "Epoch 20, CIFAR-10 Batch 1:  \tBatch: loss 2.1241 and accuracy 20.17%\t\tValid: loss 2.0457, accuracy 26.56%\n",
      "Epoch 21, CIFAR-10 Batch 1:  \tBatch: loss 2.1702 and accuracy 20.30%\t\tValid: loss 2.0334, accuracy 26.84%\n",
      "Epoch 22, CIFAR-10 Batch 1:  \tBatch: loss 2.1208 and accuracy 22.65%\t\tValid: loss 1.9995, accuracy 27.62%\n",
      "Epoch 23, CIFAR-10 Batch 1:  \tBatch: loss 2.1114 and accuracy 21.41%\t\tValid: loss 1.9824, accuracy 28.44%\n",
      "Epoch 24, CIFAR-10 Batch 1:  \tBatch: loss 2.0600 and accuracy 23.51%\t\tValid: loss 1.9683, accuracy 28.40%\n",
      "Epoch 25, CIFAR-10 Batch 1:  \tBatch: loss 2.0584 and accuracy 24.38%\t\tValid: loss 1.9452, accuracy 28.68%\n",
      "Epoch 26, CIFAR-10 Batch 1:  \tBatch: loss 2.0646 and accuracy 23.39%\t\tValid: loss 1.9363, accuracy 28.86%\n",
      "Epoch 27, CIFAR-10 Batch 1:  \tBatch: loss 2.0504 and accuracy 24.38%\t\tValid: loss 1.9234, accuracy 29.50%\n",
      "Epoch 28, CIFAR-10 Batch 1:  \tBatch: loss 1.9720 and accuracy 26.36%\t\tValid: loss 1.9031, accuracy 29.46%\n",
      "Epoch 29, CIFAR-10 Batch 1:  \tBatch: loss 2.0093 and accuracy 24.63%\t\tValid: loss 1.8688, accuracy 31.94%\n",
      "Epoch 30, CIFAR-10 Batch 1:  \tBatch: loss 2.0206 and accuracy 25.62%\t\tValid: loss 1.8842, accuracy 30.64%\n",
      "Epoch 31, CIFAR-10 Batch 1:  \tBatch: loss 1.9681 and accuracy 26.11%\t\tValid: loss 1.8434, accuracy 32.30%\n",
      "Epoch 32, CIFAR-10 Batch 1:  \tBatch: loss 1.9722 and accuracy 24.13%\t\tValid: loss 1.8323, accuracy 33.32%\n",
      "Epoch 33, CIFAR-10 Batch 1:  \tBatch: loss 1.9296 and accuracy 28.22%\t\tValid: loss 1.8247, accuracy 33.20%\n",
      "Epoch 34, CIFAR-10 Batch 1:  \tBatch: loss 1.9318 and accuracy 28.96%\t\tValid: loss 1.7852, accuracy 34.40%\n",
      "Epoch 35, CIFAR-10 Batch 1:  \tBatch: loss 1.8846 and accuracy 29.58%\t\tValid: loss 1.7804, accuracy 35.58%\n",
      "Epoch 36, CIFAR-10 Batch 1:  \tBatch: loss 1.9565 and accuracy 25.12%\t\tValid: loss 1.8263, accuracy 33.82%\n",
      "Epoch 37, CIFAR-10 Batch 1:  \tBatch: loss 1.8648 and accuracy 33.17%\t\tValid: loss 1.7692, accuracy 35.70%\n",
      "Epoch 38, CIFAR-10 Batch 1:  \tBatch: loss 1.8505 and accuracy 31.81%\t\tValid: loss 1.7436, accuracy 36.98%\n",
      "Epoch 39, CIFAR-10 Batch 1:  \tBatch: loss 1.8441 and accuracy 32.55%\t\tValid: loss 1.7353, accuracy 37.46%\n",
      "Epoch 40, CIFAR-10 Batch 1:  \tBatch: loss 1.8732 and accuracy 29.33%\t\tValid: loss 1.7505, accuracy 36.96%\n",
      "Epoch 41, CIFAR-10 Batch 1:  \tBatch: loss 1.8303 and accuracy 34.41%\t\tValid: loss 1.7020, accuracy 38.32%\n",
      "Epoch 42, CIFAR-10 Batch 1:  \tBatch: loss 1.7631 and accuracy 33.04%\t\tValid: loss 1.6987, accuracy 38.14%\n",
      "Epoch 43, CIFAR-10 Batch 1:  \tBatch: loss 1.7827 and accuracy 35.27%\t\tValid: loss 1.6867, accuracy 39.22%\n",
      "Epoch 44, CIFAR-10 Batch 1:  \tBatch: loss 1.7886 and accuracy 35.15%\t\tValid: loss 1.6595, accuracy 40.08%\n",
      "Epoch 45, CIFAR-10 Batch 1:  \tBatch: loss 1.7793 and accuracy 35.02%\t\tValid: loss 1.6849, accuracy 39.18%\n",
      "Epoch 46, CIFAR-10 Batch 1:  \tBatch: loss 1.7969 and accuracy 35.77%\t\tValid: loss 1.6538, accuracy 40.52%\n",
      "Epoch 47, CIFAR-10 Batch 1:  \tBatch: loss 1.7700 and accuracy 36.39%\t\tValid: loss 1.6699, accuracy 39.74%\n",
      "Epoch 48, CIFAR-10 Batch 1:  \tBatch: loss 1.7311 and accuracy 35.77%\t\tValid: loss 1.6235, accuracy 41.48%\n",
      "Epoch 49, CIFAR-10 Batch 1:  \tBatch: loss 1.7140 and accuracy 35.77%\t\tValid: loss 1.6328, accuracy 41.54%\n",
      "Epoch 50, CIFAR-10 Batch 1:  \tBatch: loss 1.7038 and accuracy 37.87%\t\tValid: loss 1.6163, accuracy 41.28%\n",
      "Epoch 51, CIFAR-10 Batch 1:  \tBatch: loss 1.7010 and accuracy 36.26%\t\tValid: loss 1.6072, accuracy 42.26%\n",
      "Epoch 52, CIFAR-10 Batch 1:  \tBatch: loss 1.6953 and accuracy 39.11%\t\tValid: loss 1.6097, accuracy 41.60%\n",
      "Epoch 53, CIFAR-10 Batch 1:  \tBatch: loss 1.6315 and accuracy 38.99%\t\tValid: loss 1.5812, accuracy 42.86%\n",
      "Epoch 54, CIFAR-10 Batch 1:  \tBatch: loss 1.6294 and accuracy 40.72%\t\tValid: loss 1.5877, accuracy 42.06%\n",
      "Epoch 55, CIFAR-10 Batch 1:  \tBatch: loss 1.6248 and accuracy 41.83%\t\tValid: loss 1.5625, accuracy 44.36%\n",
      "Epoch 56, CIFAR-10 Batch 1:  \tBatch: loss 1.6353 and accuracy 41.96%\t\tValid: loss 1.5675, accuracy 43.12%\n",
      "Epoch 57, CIFAR-10 Batch 1:  \tBatch: loss 1.6144 and accuracy 42.95%\t\tValid: loss 1.5485, accuracy 44.20%\n",
      "Epoch 58, CIFAR-10 Batch 1:  \tBatch: loss 1.6130 and accuracy 40.72%\t\tValid: loss 1.5351, accuracy 44.72%\n",
      "Epoch 59, CIFAR-10 Batch 1:  \tBatch: loss 1.6374 and accuracy 42.20%\t\tValid: loss 1.5720, accuracy 42.62%\n",
      "Epoch 60, CIFAR-10 Batch 1:  \tBatch: loss 1.5517 and accuracy 41.96%\t\tValid: loss 1.5189, accuracy 45.24%\n",
      "Epoch 61, CIFAR-10 Batch 1:  \tBatch: loss 1.5741 and accuracy 42.33%\t\tValid: loss 1.5242, accuracy 44.94%\n",
      "Epoch 62, CIFAR-10 Batch 1:  \tBatch: loss 1.5296 and accuracy 42.45%\t\tValid: loss 1.5218, accuracy 44.96%\n",
      "Epoch 63, CIFAR-10 Batch 1:  \tBatch: loss 1.5334 and accuracy 45.42%\t\tValid: loss 1.5099, accuracy 45.58%\n",
      "Epoch 64, CIFAR-10 Batch 1:  \tBatch: loss 1.4999 and accuracy 46.04%\t\tValid: loss 1.5101, accuracy 45.46%\n",
      "Epoch 65, CIFAR-10 Batch 1:  \tBatch: loss 1.4967 and accuracy 45.67%\t\tValid: loss 1.5096, accuracy 45.48%\n",
      "Epoch 66, CIFAR-10 Batch 1:  \tBatch: loss 1.4767 and accuracy 46.41%\t\tValid: loss 1.4903, accuracy 46.40%\n",
      "Epoch 67, CIFAR-10 Batch 1:  \tBatch: loss 1.4509 and accuracy 48.51%\t\tValid: loss 1.4917, accuracy 46.64%\n",
      "Epoch 68, CIFAR-10 Batch 1:  \tBatch: loss 1.4652 and accuracy 47.52%\t\tValid: loss 1.5020, accuracy 45.54%\n",
      "Epoch 69, CIFAR-10 Batch 1:  \tBatch: loss 1.4801 and accuracy 46.53%\t\tValid: loss 1.5018, accuracy 45.88%\n",
      "Epoch 70, CIFAR-10 Batch 1:  \tBatch: loss 1.4307 and accuracy 47.90%\t\tValid: loss 1.4699, accuracy 46.88%\n",
      "Epoch 71, CIFAR-10 Batch 1:  \tBatch: loss 1.4094 and accuracy 48.64%\t\tValid: loss 1.4765, accuracy 46.38%\n",
      "Epoch 72, CIFAR-10 Batch 1:  \tBatch: loss 1.3828 and accuracy 50.87%\t\tValid: loss 1.4614, accuracy 47.04%\n",
      "Epoch 73, CIFAR-10 Batch 1:  \tBatch: loss 1.4184 and accuracy 49.63%\t\tValid: loss 1.4584, accuracy 46.84%\n",
      "Epoch 74, CIFAR-10 Batch 1:  \tBatch: loss 1.3832 and accuracy 48.76%\t\tValid: loss 1.4801, accuracy 46.74%\n",
      "Epoch 75, CIFAR-10 Batch 1:  \tBatch: loss 1.4247 and accuracy 47.90%\t\tValid: loss 1.4673, accuracy 46.72%\n",
      "Epoch 76, CIFAR-10 Batch 1:  \tBatch: loss 1.3619 and accuracy 50.99%\t\tValid: loss 1.4561, accuracy 47.34%\n",
      "Epoch 77, CIFAR-10 Batch 1:  \tBatch: loss 1.3346 and accuracy 51.49%\t\tValid: loss 1.4505, accuracy 47.64%\n",
      "Epoch 78, CIFAR-10 Batch 1:  \tBatch: loss 1.3364 and accuracy 51.61%\t\tValid: loss 1.4624, accuracy 47.34%\n",
      "Epoch 79, CIFAR-10 Batch 1:  \tBatch: loss 1.3336 and accuracy 52.48%\t\tValid: loss 1.4496, accuracy 48.14%\n",
      "Epoch 80, CIFAR-10 Batch 1:  \tBatch: loss 1.2981 and accuracy 53.34%\t\tValid: loss 1.4415, accuracy 47.62%\n",
      "Epoch 81, CIFAR-10 Batch 1:  \tBatch: loss 1.3455 and accuracy 52.97%\t\tValid: loss 1.4981, accuracy 45.98%\n",
      "Epoch 82, CIFAR-10 Batch 1:  \tBatch: loss 1.3021 and accuracy 54.08%\t\tValid: loss 1.4643, accuracy 46.84%\n",
      "Epoch 83, CIFAR-10 Batch 1:  \tBatch: loss 1.3470 and accuracy 50.74%\t\tValid: loss 1.4509, accuracy 48.26%\n",
      "Epoch 84, CIFAR-10 Batch 1:  \tBatch: loss 1.2744 and accuracy 56.19%\t\tValid: loss 1.4397, accuracy 47.84%\n",
      "Epoch 85, CIFAR-10 Batch 1:  \tBatch: loss 1.2257 and accuracy 56.81%\t\tValid: loss 1.4802, accuracy 48.32%\n",
      "Epoch 86, CIFAR-10 Batch 1:  \tBatch: loss 1.1813 and accuracy 56.81%\t\tValid: loss 1.4305, accuracy 48.82%\n",
      "Epoch 87, CIFAR-10 Batch 1:  \tBatch: loss 1.1921 and accuracy 55.32%\t\tValid: loss 1.4308, accuracy 49.08%\n",
      "Epoch 88, CIFAR-10 Batch 1:  \tBatch: loss 1.2368 and accuracy 56.06%\t\tValid: loss 1.4754, accuracy 47.88%\n",
      "Epoch 89, CIFAR-10 Batch 1:  \tBatch: loss 1.3116 and accuracy 51.98%\t\tValid: loss 1.5160, accuracy 47.28%\n",
      "Epoch 90, CIFAR-10 Batch 1:  \tBatch: loss 1.2027 and accuracy 56.06%\t\tValid: loss 1.4529, accuracy 49.14%\n",
      "Epoch 91, CIFAR-10 Batch 1:  \tBatch: loss 1.1881 and accuracy 56.56%\t\tValid: loss 1.4472, accuracy 48.64%\n",
      "Epoch 92, CIFAR-10 Batch 1:  \tBatch: loss 1.1677 and accuracy 56.81%\t\tValid: loss 1.4647, accuracy 48.60%\n",
      "Epoch 93, CIFAR-10 Batch 1:  \tBatch: loss 1.1819 and accuracy 58.66%\t\tValid: loss 1.4544, accuracy 49.06%\n",
      "Epoch 94, CIFAR-10 Batch 1:  \tBatch: loss 1.1411 and accuracy 56.81%\t\tValid: loss 1.4751, accuracy 48.94%\n",
      "Epoch 95, CIFAR-10 Batch 1:  \tBatch: loss 1.1810 and accuracy 58.04%\t\tValid: loss 1.4414, accuracy 49.08%\n",
      "Epoch 96, CIFAR-10 Batch 1:  \tBatch: loss 1.1606 and accuracy 59.41%\t\tValid: loss 1.4858, accuracy 48.38%\n",
      "Epoch 97, CIFAR-10 Batch 1:  \tBatch: loss 1.1537 and accuracy 58.91%\t\tValid: loss 1.4658, accuracy 48.82%\n",
      "Epoch 98, CIFAR-10 Batch 1:  \tBatch: loss 1.0770 and accuracy 60.77%\t\tValid: loss 1.4324, accuracy 49.42%\n",
      "Epoch 99, CIFAR-10 Batch 1:  \tBatch: loss 1.0730 and accuracy 61.39%\t\tValid: loss 1.4468, accuracy 50.24%\n",
      "Epoch 100, CIFAR-10 Batch 1:  \tBatch: loss 1.0511 and accuracy 62.75%\t\tValid: loss 1.4492, accuracy 50.34%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \tBatch: loss 3.7802 and accuracy 10.52%\t\tValid: loss 2.4031, accuracy 9.76%\n",
      "Epoch  1, CIFAR-10 Batch 2:  \tBatch: loss 2.8251 and accuracy 9.90%\t\tValid: loss 2.3397, accuracy 8.94%\n",
      "Epoch  1, CIFAR-10 Batch 3:  \tBatch: loss 2.6240 and accuracy 11.51%\t\tValid: loss 2.3178, accuracy 9.86%\n",
      "Epoch  1, CIFAR-10 Batch 4:  \tBatch: loss 2.6041 and accuracy 10.27%\t\tValid: loss 2.2986, accuracy 11.10%\n",
      "Epoch  1, CIFAR-10 Batch 5:  \tBatch: loss 2.5266 and accuracy 11.76%\t\tValid: loss 2.2703, accuracy 14.86%\n",
      "Epoch  2, CIFAR-10 Batch 1:  \tBatch: loss 2.4962 and accuracy 10.89%\t\tValid: loss 2.2427, accuracy 16.90%\n",
      "Epoch  2, CIFAR-10 Batch 2:  \tBatch: loss 2.5027 and accuracy 11.01%\t\tValid: loss 2.2140, accuracy 19.00%\n",
      "Epoch  2, CIFAR-10 Batch 3:  \tBatch: loss 2.4134 and accuracy 14.23%\t\tValid: loss 2.1949, accuracy 18.78%\n",
      "Epoch  2, CIFAR-10 Batch 4:  \tBatch: loss 2.3467 and accuracy 14.73%\t\tValid: loss 2.1533, accuracy 20.34%\n",
      "Epoch  2, CIFAR-10 Batch 5:  \tBatch: loss 2.2914 and accuracy 16.83%\t\tValid: loss 2.1168, accuracy 20.98%\n",
      "Epoch  3, CIFAR-10 Batch 1:  \tBatch: loss 2.3087 and accuracy 13.99%\t\tValid: loss 2.0818, accuracy 23.88%\n",
      "Epoch  3, CIFAR-10 Batch 2:  \tBatch: loss 2.2514 and accuracy 17.82%\t\tValid: loss 2.0751, accuracy 23.86%\n",
      "Epoch  3, CIFAR-10 Batch 3:  \tBatch: loss 2.2430 and accuracy 15.84%\t\tValid: loss 2.0275, accuracy 25.82%\n",
      "Epoch  3, CIFAR-10 Batch 4:  \tBatch: loss 2.1876 and accuracy 17.57%\t\tValid: loss 2.0329, accuracy 24.22%\n",
      "Epoch  3, CIFAR-10 Batch 5:  \tBatch: loss 2.1109 and accuracy 21.53%\t\tValid: loss 2.0010, accuracy 25.80%\n",
      "Epoch  4, CIFAR-10 Batch 1:  \tBatch: loss 2.1340 and accuracy 20.42%\t\tValid: loss 1.9914, accuracy 28.98%\n",
      "Epoch  4, CIFAR-10 Batch 2:  \tBatch: loss 2.1394 and accuracy 19.31%\t\tValid: loss 1.9871, accuracy 26.80%\n",
      "Epoch  4, CIFAR-10 Batch 3:  \tBatch: loss 2.1075 and accuracy 19.43%\t\tValid: loss 1.9283, accuracy 29.52%\n",
      "Epoch  4, CIFAR-10 Batch 4:  \tBatch: loss 2.0613 and accuracy 20.79%\t\tValid: loss 1.9387, accuracy 28.30%\n",
      "Epoch  4, CIFAR-10 Batch 5:  \tBatch: loss 2.0582 and accuracy 24.50%\t\tValid: loss 1.9128, accuracy 30.36%\n",
      "Epoch  5, CIFAR-10 Batch 1:  \tBatch: loss 2.0552 and accuracy 24.01%\t\tValid: loss 1.8931, accuracy 31.38%\n",
      "Epoch  5, CIFAR-10 Batch 2:  \tBatch: loss 2.0084 and accuracy 25.50%\t\tValid: loss 1.8965, accuracy 31.88%\n",
      "Epoch  5, CIFAR-10 Batch 3:  \tBatch: loss 2.0510 and accuracy 24.38%\t\tValid: loss 1.8548, accuracy 32.72%\n",
      "Epoch  5, CIFAR-10 Batch 4:  \tBatch: loss 1.9827 and accuracy 24.38%\t\tValid: loss 1.8720, accuracy 32.80%\n",
      "Epoch  5, CIFAR-10 Batch 5:  \tBatch: loss 1.9901 and accuracy 25.00%\t\tValid: loss 1.8296, accuracy 35.54%\n",
      "Epoch  6, CIFAR-10 Batch 1:  \tBatch: loss 1.9634 and accuracy 27.85%\t\tValid: loss 1.8102, accuracy 36.28%\n",
      "Epoch  6, CIFAR-10 Batch 2:  \tBatch: loss 1.9773 and accuracy 28.84%\t\tValid: loss 1.8272, accuracy 35.02%\n",
      "Epoch  6, CIFAR-10 Batch 3:  \tBatch: loss 1.8890 and accuracy 29.95%\t\tValid: loss 1.7782, accuracy 35.82%\n",
      "Epoch  6, CIFAR-10 Batch 4:  \tBatch: loss 1.8846 and accuracy 28.09%\t\tValid: loss 1.7728, accuracy 36.64%\n",
      "Epoch  6, CIFAR-10 Batch 5:  \tBatch: loss 1.9097 and accuracy 26.86%\t\tValid: loss 1.7559, accuracy 37.56%\n",
      "Epoch  7, CIFAR-10 Batch 1:  \tBatch: loss 1.9009 and accuracy 29.70%\t\tValid: loss 1.7369, accuracy 39.16%\n",
      "Epoch  7, CIFAR-10 Batch 2:  \tBatch: loss 1.8954 and accuracy 27.85%\t\tValid: loss 1.7209, accuracy 38.78%\n",
      "Epoch  7, CIFAR-10 Batch 3:  \tBatch: loss 1.8405 and accuracy 32.05%\t\tValid: loss 1.7062, accuracy 38.84%\n",
      "Epoch  7, CIFAR-10 Batch 4:  \tBatch: loss 1.8111 and accuracy 33.17%\t\tValid: loss 1.6927, accuracy 39.82%\n",
      "Epoch  7, CIFAR-10 Batch 5:  \tBatch: loss 1.8746 and accuracy 30.94%\t\tValid: loss 1.7024, accuracy 39.02%\n",
      "Epoch  8, CIFAR-10 Batch 1:  \tBatch: loss 1.8508 and accuracy 29.21%\t\tValid: loss 1.6664, accuracy 41.22%\n",
      "Epoch  8, CIFAR-10 Batch 2:  \tBatch: loss 1.8293 and accuracy 31.81%\t\tValid: loss 1.6559, accuracy 40.00%\n",
      "Epoch  8, CIFAR-10 Batch 3:  \tBatch: loss 1.7851 and accuracy 35.02%\t\tValid: loss 1.6468, accuracy 41.20%\n",
      "Epoch  8, CIFAR-10 Batch 4:  \tBatch: loss 1.7665 and accuracy 35.02%\t\tValid: loss 1.6392, accuracy 41.32%\n",
      "Epoch  8, CIFAR-10 Batch 5:  \tBatch: loss 1.7915 and accuracy 36.88%\t\tValid: loss 1.6310, accuracy 41.42%\n",
      "Epoch  9, CIFAR-10 Batch 1:  \tBatch: loss 1.7715 and accuracy 35.52%\t\tValid: loss 1.6035, accuracy 42.30%\n",
      "Epoch  9, CIFAR-10 Batch 2:  \tBatch: loss 1.7844 and accuracy 36.88%\t\tValid: loss 1.6059, accuracy 42.42%\n",
      "Epoch  9, CIFAR-10 Batch 3:  \tBatch: loss 1.7247 and accuracy 35.64%\t\tValid: loss 1.6234, accuracy 41.78%\n",
      "Epoch  9, CIFAR-10 Batch 4:  \tBatch: loss 1.7341 and accuracy 38.00%\t\tValid: loss 1.6207, accuracy 41.00%\n",
      "Epoch  9, CIFAR-10 Batch 5:  \tBatch: loss 1.7109 and accuracy 38.00%\t\tValid: loss 1.5805, accuracy 42.98%\n",
      "Epoch 10, CIFAR-10 Batch 1:  \tBatch: loss 1.7094 and accuracy 38.12%\t\tValid: loss 1.5636, accuracy 43.62%\n",
      "Epoch 10, CIFAR-10 Batch 2:  \tBatch: loss 1.6985 and accuracy 38.37%\t\tValid: loss 1.5880, accuracy 43.08%\n",
      "Epoch 10, CIFAR-10 Batch 3:  \tBatch: loss 1.6417 and accuracy 37.75%\t\tValid: loss 1.5518, accuracy 43.50%\n",
      "Epoch 10, CIFAR-10 Batch 4:  \tBatch: loss 1.6228 and accuracy 39.85%\t\tValid: loss 1.5477, accuracy 43.90%\n",
      "Epoch 10, CIFAR-10 Batch 5:  \tBatch: loss 1.6923 and accuracy 38.12%\t\tValid: loss 1.5337, accuracy 44.74%\n",
      "Epoch 11, CIFAR-10 Batch 1:  \tBatch: loss 1.7054 and accuracy 38.49%\t\tValid: loss 1.5645, accuracy 43.34%\n",
      "Epoch 11, CIFAR-10 Batch 2:  \tBatch: loss 1.6722 and accuracy 38.61%\t\tValid: loss 1.5244, accuracy 44.78%\n",
      "Epoch 11, CIFAR-10 Batch 3:  \tBatch: loss 1.6431 and accuracy 38.37%\t\tValid: loss 1.5264, accuracy 44.46%\n",
      "Epoch 11, CIFAR-10 Batch 4:  \tBatch: loss 1.6643 and accuracy 41.21%\t\tValid: loss 1.5163, accuracy 44.94%\n",
      "Epoch 11, CIFAR-10 Batch 5:  \tBatch: loss 1.6493 and accuracy 39.11%\t\tValid: loss 1.5110, accuracy 45.20%\n",
      "Epoch 12, CIFAR-10 Batch 1:  \tBatch: loss 1.6191 and accuracy 41.71%\t\tValid: loss 1.5057, accuracy 45.36%\n",
      "Epoch 12, CIFAR-10 Batch 2:  \tBatch: loss 1.6256 and accuracy 41.96%\t\tValid: loss 1.4937, accuracy 46.06%\n",
      "Epoch 12, CIFAR-10 Batch 3:  \tBatch: loss 1.5705 and accuracy 40.72%\t\tValid: loss 1.4980, accuracy 45.38%\n",
      "Epoch 12, CIFAR-10 Batch 4:  \tBatch: loss 1.5738 and accuracy 42.45%\t\tValid: loss 1.4859, accuracy 45.26%\n",
      "Epoch 12, CIFAR-10 Batch 5:  \tBatch: loss 1.6424 and accuracy 38.37%\t\tValid: loss 1.5069, accuracy 44.48%\n",
      "Epoch 13, CIFAR-10 Batch 1:  \tBatch: loss 1.6404 and accuracy 41.34%\t\tValid: loss 1.4924, accuracy 45.78%\n",
      "Epoch 13, CIFAR-10 Batch 2:  \tBatch: loss 1.5780 and accuracy 41.46%\t\tValid: loss 1.4694, accuracy 47.18%\n",
      "Epoch 13, CIFAR-10 Batch 3:  \tBatch: loss 1.5301 and accuracy 43.69%\t\tValid: loss 1.4525, accuracy 47.26%\n",
      "Epoch 13, CIFAR-10 Batch 4:  \tBatch: loss 1.5368 and accuracy 46.53%\t\tValid: loss 1.4513, accuracy 46.74%\n",
      "Epoch 13, CIFAR-10 Batch 5:  \tBatch: loss 1.6142 and accuracy 42.70%\t\tValid: loss 1.4820, accuracy 45.52%\n",
      "Epoch 14, CIFAR-10 Batch 1:  \tBatch: loss 1.6012 and accuracy 42.57%\t\tValid: loss 1.4770, accuracy 45.08%\n",
      "Epoch 14, CIFAR-10 Batch 2:  \tBatch: loss 1.6037 and accuracy 42.45%\t\tValid: loss 1.4810, accuracy 46.84%\n",
      "Epoch 14, CIFAR-10 Batch 3:  \tBatch: loss 1.5802 and accuracy 41.46%\t\tValid: loss 1.4722, accuracy 46.26%\n",
      "Epoch 14, CIFAR-10 Batch 4:  \tBatch: loss 1.5538 and accuracy 45.30%\t\tValid: loss 1.4641, accuracy 46.34%\n",
      "Epoch 14, CIFAR-10 Batch 5:  \tBatch: loss 1.6101 and accuracy 42.82%\t\tValid: loss 1.4380, accuracy 47.70%\n",
      "Epoch 15, CIFAR-10 Batch 1:  \tBatch: loss 1.5468 and accuracy 42.95%\t\tValid: loss 1.4193, accuracy 48.30%\n",
      "Epoch 15, CIFAR-10 Batch 2:  \tBatch: loss 1.5018 and accuracy 47.40%\t\tValid: loss 1.4280, accuracy 48.38%\n",
      "Epoch 15, CIFAR-10 Batch 3:  \tBatch: loss 1.5144 and accuracy 45.05%\t\tValid: loss 1.4122, accuracy 48.14%\n",
      "Epoch 15, CIFAR-10 Batch 4:  \tBatch: loss 1.4988 and accuracy 45.42%\t\tValid: loss 1.4187, accuracy 48.14%\n",
      "Epoch 15, CIFAR-10 Batch 5:  \tBatch: loss 1.5016 and accuracy 46.91%\t\tValid: loss 1.4082, accuracy 49.18%\n",
      "Epoch 16, CIFAR-10 Batch 1:  \tBatch: loss 1.4935 and accuracy 46.91%\t\tValid: loss 1.3948, accuracy 48.72%\n",
      "Epoch 16, CIFAR-10 Batch 2:  \tBatch: loss 1.5080 and accuracy 46.41%\t\tValid: loss 1.4081, accuracy 48.94%\n",
      "Epoch 16, CIFAR-10 Batch 3:  \tBatch: loss 1.4900 and accuracy 45.30%\t\tValid: loss 1.4019, accuracy 48.28%\n",
      "Epoch 16, CIFAR-10 Batch 4:  \tBatch: loss 1.4638 and accuracy 48.51%\t\tValid: loss 1.4095, accuracy 48.62%\n",
      "Epoch 16, CIFAR-10 Batch 5:  \tBatch: loss 1.5227 and accuracy 44.93%\t\tValid: loss 1.3917, accuracy 49.64%\n",
      "Epoch 17, CIFAR-10 Batch 1:  \tBatch: loss 1.5092 and accuracy 47.40%\t\tValid: loss 1.3807, accuracy 49.48%\n",
      "Epoch 17, CIFAR-10 Batch 2:  \tBatch: loss 1.4898 and accuracy 49.63%\t\tValid: loss 1.3932, accuracy 49.30%\n",
      "Epoch 17, CIFAR-10 Batch 3:  \tBatch: loss 1.4535 and accuracy 46.53%\t\tValid: loss 1.3842, accuracy 48.98%\n",
      "Epoch 17, CIFAR-10 Batch 4:  \tBatch: loss 1.4428 and accuracy 49.75%\t\tValid: loss 1.3716, accuracy 49.48%\n",
      "Epoch 17, CIFAR-10 Batch 5:  \tBatch: loss 1.4855 and accuracy 47.77%\t\tValid: loss 1.3660, accuracy 50.70%\n",
      "Epoch 18, CIFAR-10 Batch 1:  \tBatch: loss 1.4808 and accuracy 47.65%\t\tValid: loss 1.3665, accuracy 49.76%\n",
      "Epoch 18, CIFAR-10 Batch 2:  \tBatch: loss 1.4482 and accuracy 47.40%\t\tValid: loss 1.3550, accuracy 50.78%\n",
      "Epoch 18, CIFAR-10 Batch 3:  \tBatch: loss 1.4248 and accuracy 47.77%\t\tValid: loss 1.3836, accuracy 49.06%\n",
      "Epoch 18, CIFAR-10 Batch 4:  \tBatch: loss 1.4338 and accuracy 49.38%\t\tValid: loss 1.3703, accuracy 49.64%\n",
      "Epoch 18, CIFAR-10 Batch 5:  \tBatch: loss 1.4922 and accuracy 47.40%\t\tValid: loss 1.3826, accuracy 50.46%\n",
      "Epoch 19, CIFAR-10 Batch 1:  \tBatch: loss 1.4306 and accuracy 51.24%\t\tValid: loss 1.3598, accuracy 49.80%\n",
      "Epoch 19, CIFAR-10 Batch 2:  \tBatch: loss 1.4770 and accuracy 46.53%\t\tValid: loss 1.3699, accuracy 50.02%\n",
      "Epoch 19, CIFAR-10 Batch 3:  \tBatch: loss 1.4238 and accuracy 47.90%\t\tValid: loss 1.3979, accuracy 48.68%\n",
      "Epoch 19, CIFAR-10 Batch 4:  \tBatch: loss 1.4208 and accuracy 50.25%\t\tValid: loss 1.3589, accuracy 50.52%\n",
      "Epoch 19, CIFAR-10 Batch 5:  \tBatch: loss 1.4535 and accuracy 50.87%\t\tValid: loss 1.3625, accuracy 50.94%\n",
      "Epoch 20, CIFAR-10 Batch 1:  \tBatch: loss 1.4160 and accuracy 48.51%\t\tValid: loss 1.3615, accuracy 49.90%\n",
      "Epoch 20, CIFAR-10 Batch 2:  \tBatch: loss 1.4499 and accuracy 49.50%\t\tValid: loss 1.3582, accuracy 51.04%\n",
      "Epoch 20, CIFAR-10 Batch 3:  \tBatch: loss 1.4214 and accuracy 49.01%\t\tValid: loss 1.3815, accuracy 49.80%\n",
      "Epoch 20, CIFAR-10 Batch 4:  \tBatch: loss 1.4138 and accuracy 50.00%\t\tValid: loss 1.3542, accuracy 50.66%\n",
      "Epoch 20, CIFAR-10 Batch 5:  \tBatch: loss 1.4084 and accuracy 48.51%\t\tValid: loss 1.3307, accuracy 51.42%\n",
      "Epoch 21, CIFAR-10 Batch 1:  \tBatch: loss 1.4053 and accuracy 51.49%\t\tValid: loss 1.3175, accuracy 51.48%\n",
      "Epoch 21, CIFAR-10 Batch 2:  \tBatch: loss 1.4127 and accuracy 51.24%\t\tValid: loss 1.3239, accuracy 51.98%\n",
      "Epoch 21, CIFAR-10 Batch 3:  \tBatch: loss 1.3797 and accuracy 51.24%\t\tValid: loss 1.3430, accuracy 50.68%\n",
      "Epoch 21, CIFAR-10 Batch 4:  \tBatch: loss 1.3497 and accuracy 51.86%\t\tValid: loss 1.3156, accuracy 51.86%\n",
      "Epoch 21, CIFAR-10 Batch 5:  \tBatch: loss 1.3681 and accuracy 50.99%\t\tValid: loss 1.3090, accuracy 52.46%\n",
      "Epoch 22, CIFAR-10 Batch 1:  \tBatch: loss 1.3914 and accuracy 52.48%\t\tValid: loss 1.3126, accuracy 51.72%\n",
      "Epoch 22, CIFAR-10 Batch 2:  \tBatch: loss 1.4176 and accuracy 49.63%\t\tValid: loss 1.3131, accuracy 52.22%\n",
      "Epoch 22, CIFAR-10 Batch 3:  \tBatch: loss 1.3444 and accuracy 50.25%\t\tValid: loss 1.3034, accuracy 51.92%\n",
      "Epoch 22, CIFAR-10 Batch 4:  \tBatch: loss 1.3669 and accuracy 53.71%\t\tValid: loss 1.3183, accuracy 51.94%\n",
      "Epoch 22, CIFAR-10 Batch 5:  \tBatch: loss 1.3658 and accuracy 51.61%\t\tValid: loss 1.2946, accuracy 52.60%\n",
      "Epoch 23, CIFAR-10 Batch 1:  \tBatch: loss 1.3783 and accuracy 49.88%\t\tValid: loss 1.3054, accuracy 51.90%\n",
      "Epoch 23, CIFAR-10 Batch 2:  \tBatch: loss 1.3680 and accuracy 54.21%\t\tValid: loss 1.3115, accuracy 52.58%\n",
      "Epoch 23, CIFAR-10 Batch 3:  \tBatch: loss 1.3246 and accuracy 52.60%\t\tValid: loss 1.2998, accuracy 51.88%\n",
      "Epoch 23, CIFAR-10 Batch 4:  \tBatch: loss 1.3161 and accuracy 53.59%\t\tValid: loss 1.2874, accuracy 52.60%\n",
      "Epoch 23, CIFAR-10 Batch 5:  \tBatch: loss 1.3008 and accuracy 53.84%\t\tValid: loss 1.2817, accuracy 52.94%\n",
      "Epoch 24, CIFAR-10 Batch 1:  \tBatch: loss 1.3702 and accuracy 52.72%\t\tValid: loss 1.3100, accuracy 52.68%\n",
      "Epoch 24, CIFAR-10 Batch 2:  \tBatch: loss 1.3555 and accuracy 49.38%\t\tValid: loss 1.2814, accuracy 53.12%\n",
      "Epoch 24, CIFAR-10 Batch 3:  \tBatch: loss 1.3317 and accuracy 51.86%\t\tValid: loss 1.2926, accuracy 53.04%\n",
      "Epoch 24, CIFAR-10 Batch 4:  \tBatch: loss 1.3014 and accuracy 56.06%\t\tValid: loss 1.2844, accuracy 53.04%\n",
      "Epoch 24, CIFAR-10 Batch 5:  \tBatch: loss 1.3077 and accuracy 53.59%\t\tValid: loss 1.2680, accuracy 53.62%\n",
      "Epoch 25, CIFAR-10 Batch 1:  \tBatch: loss 1.3113 and accuracy 56.06%\t\tValid: loss 1.2704, accuracy 53.10%\n",
      "Epoch 25, CIFAR-10 Batch 2:  \tBatch: loss 1.3321 and accuracy 53.22%\t\tValid: loss 1.2726, accuracy 53.82%\n",
      "Epoch 25, CIFAR-10 Batch 3:  \tBatch: loss 1.3386 and accuracy 50.74%\t\tValid: loss 1.2840, accuracy 53.00%\n",
      "Epoch 25, CIFAR-10 Batch 4:  \tBatch: loss 1.2727 and accuracy 55.45%\t\tValid: loss 1.2768, accuracy 53.54%\n",
      "Epoch 25, CIFAR-10 Batch 5:  \tBatch: loss 1.2961 and accuracy 54.08%\t\tValid: loss 1.2568, accuracy 53.80%\n",
      "Epoch 26, CIFAR-10 Batch 1:  \tBatch: loss 1.3187 and accuracy 52.97%\t\tValid: loss 1.2665, accuracy 53.20%\n",
      "Epoch 26, CIFAR-10 Batch 2:  \tBatch: loss 1.3318 and accuracy 51.73%\t\tValid: loss 1.2735, accuracy 54.14%\n",
      "Epoch 26, CIFAR-10 Batch 3:  \tBatch: loss 1.2670 and accuracy 54.95%\t\tValid: loss 1.2652, accuracy 54.16%\n",
      "Epoch 26, CIFAR-10 Batch 4:  \tBatch: loss 1.2502 and accuracy 55.32%\t\tValid: loss 1.2784, accuracy 52.72%\n",
      "Epoch 26, CIFAR-10 Batch 5:  \tBatch: loss 1.2998 and accuracy 55.32%\t\tValid: loss 1.2555, accuracy 54.14%\n",
      "Epoch 27, CIFAR-10 Batch 1:  \tBatch: loss 1.3520 and accuracy 52.10%\t\tValid: loss 1.2736, accuracy 54.16%\n",
      "Epoch 27, CIFAR-10 Batch 2:  \tBatch: loss 1.3312 and accuracy 52.23%\t\tValid: loss 1.2910, accuracy 53.36%\n",
      "Epoch 27, CIFAR-10 Batch 3:  \tBatch: loss 1.2509 and accuracy 52.85%\t\tValid: loss 1.2542, accuracy 54.20%\n",
      "Epoch 27, CIFAR-10 Batch 4:  \tBatch: loss 1.2510 and accuracy 56.19%\t\tValid: loss 1.2470, accuracy 53.92%\n",
      "Epoch 27, CIFAR-10 Batch 5:  \tBatch: loss 1.3011 and accuracy 53.96%\t\tValid: loss 1.2390, accuracy 54.42%\n",
      "Epoch 28, CIFAR-10 Batch 1:  \tBatch: loss 1.2993 and accuracy 52.60%\t\tValid: loss 1.2446, accuracy 54.38%\n",
      "Epoch 28, CIFAR-10 Batch 2:  \tBatch: loss 1.2834 and accuracy 56.06%\t\tValid: loss 1.2589, accuracy 54.38%\n",
      "Epoch 28, CIFAR-10 Batch 3:  \tBatch: loss 1.2443 and accuracy 54.21%\t\tValid: loss 1.2357, accuracy 54.56%\n",
      "Epoch 28, CIFAR-10 Batch 4:  \tBatch: loss 1.2656 and accuracy 54.83%\t\tValid: loss 1.2586, accuracy 54.10%\n",
      "Epoch 28, CIFAR-10 Batch 5:  \tBatch: loss 1.2468 and accuracy 55.20%\t\tValid: loss 1.2498, accuracy 54.08%\n",
      "Epoch 29, CIFAR-10 Batch 1:  \tBatch: loss 1.2626 and accuracy 56.68%\t\tValid: loss 1.2419, accuracy 54.54%\n",
      "Epoch 29, CIFAR-10 Batch 2:  \tBatch: loss 1.2871 and accuracy 56.44%\t\tValid: loss 1.2633, accuracy 54.30%\n",
      "Epoch 29, CIFAR-10 Batch 3:  \tBatch: loss 1.1962 and accuracy 57.05%\t\tValid: loss 1.2387, accuracy 54.90%\n",
      "Epoch 29, CIFAR-10 Batch 4:  \tBatch: loss 1.2724 and accuracy 54.08%\t\tValid: loss 1.2406, accuracy 54.56%\n",
      "Epoch 29, CIFAR-10 Batch 5:  \tBatch: loss 1.2269 and accuracy 57.67%\t\tValid: loss 1.2279, accuracy 55.24%\n",
      "Epoch 30, CIFAR-10 Batch 1:  \tBatch: loss 1.2632 and accuracy 53.59%\t\tValid: loss 1.2290, accuracy 55.00%\n",
      "Epoch 30, CIFAR-10 Batch 2:  \tBatch: loss 1.2318 and accuracy 56.19%\t\tValid: loss 1.2336, accuracy 55.08%\n",
      "Epoch 30, CIFAR-10 Batch 3:  \tBatch: loss 1.2085 and accuracy 56.56%\t\tValid: loss 1.2331, accuracy 55.84%\n",
      "Epoch 30, CIFAR-10 Batch 4:  \tBatch: loss 1.1885 and accuracy 58.04%\t\tValid: loss 1.2212, accuracy 55.28%\n",
      "Epoch 30, CIFAR-10 Batch 5:  \tBatch: loss 1.1860 and accuracy 59.16%\t\tValid: loss 1.2118, accuracy 55.32%\n",
      "Epoch 31, CIFAR-10 Batch 1:  \tBatch: loss 1.2043 and accuracy 57.55%\t\tValid: loss 1.2176, accuracy 55.26%\n",
      "Epoch 31, CIFAR-10 Batch 2:  \tBatch: loss 1.2260 and accuracy 55.20%\t\tValid: loss 1.2475, accuracy 55.04%\n",
      "Epoch 31, CIFAR-10 Batch 3:  \tBatch: loss 1.1880 and accuracy 57.67%\t\tValid: loss 1.2161, accuracy 55.96%\n",
      "Epoch 31, CIFAR-10 Batch 4:  \tBatch: loss 1.2504 and accuracy 55.45%\t\tValid: loss 1.2488, accuracy 55.12%\n",
      "Epoch 31, CIFAR-10 Batch 5:  \tBatch: loss 1.2250 and accuracy 55.69%\t\tValid: loss 1.2294, accuracy 55.46%\n",
      "Epoch 32, CIFAR-10 Batch 1:  \tBatch: loss 1.2238 and accuracy 57.67%\t\tValid: loss 1.2067, accuracy 56.38%\n",
      "Epoch 32, CIFAR-10 Batch 2:  \tBatch: loss 1.2142 and accuracy 57.92%\t\tValid: loss 1.2090, accuracy 55.74%\n",
      "Epoch 32, CIFAR-10 Batch 3:  \tBatch: loss 1.2215 and accuracy 55.82%\t\tValid: loss 1.2478, accuracy 55.98%\n",
      "Epoch 32, CIFAR-10 Batch 4:  \tBatch: loss 1.2235 and accuracy 55.45%\t\tValid: loss 1.2163, accuracy 55.90%\n",
      "Epoch 32, CIFAR-10 Batch 5:  \tBatch: loss 1.1879 and accuracy 60.02%\t\tValid: loss 1.1967, accuracy 56.12%\n",
      "Epoch 33, CIFAR-10 Batch 1:  \tBatch: loss 1.1884 and accuracy 58.91%\t\tValid: loss 1.2113, accuracy 56.32%\n",
      "Epoch 33, CIFAR-10 Batch 2:  \tBatch: loss 1.1942 and accuracy 59.03%\t\tValid: loss 1.1981, accuracy 56.02%\n",
      "Epoch 33, CIFAR-10 Batch 3:  \tBatch: loss 1.1790 and accuracy 57.92%\t\tValid: loss 1.2251, accuracy 56.12%\n",
      "Epoch 33, CIFAR-10 Batch 4:  \tBatch: loss 1.1946 and accuracy 56.44%\t\tValid: loss 1.2191, accuracy 55.52%\n",
      "Epoch 33, CIFAR-10 Batch 5:  \tBatch: loss 1.2046 and accuracy 58.79%\t\tValid: loss 1.2069, accuracy 55.78%\n",
      "Epoch 34, CIFAR-10 Batch 1:  \tBatch: loss 1.2065 and accuracy 56.93%\t\tValid: loss 1.2159, accuracy 57.38%\n",
      "Epoch 34, CIFAR-10 Batch 2:  \tBatch: loss 1.1514 and accuracy 58.66%\t\tValid: loss 1.1849, accuracy 56.68%\n",
      "Epoch 34, CIFAR-10 Batch 3:  \tBatch: loss 1.1697 and accuracy 60.52%\t\tValid: loss 1.2079, accuracy 56.58%\n",
      "Epoch 34, CIFAR-10 Batch 4:  \tBatch: loss 1.1491 and accuracy 59.03%\t\tValid: loss 1.2162, accuracy 55.92%\n",
      "Epoch 34, CIFAR-10 Batch 5:  \tBatch: loss 1.1309 and accuracy 62.13%\t\tValid: loss 1.1781, accuracy 57.18%\n",
      "Epoch 35, CIFAR-10 Batch 1:  \tBatch: loss 1.1465 and accuracy 59.41%\t\tValid: loss 1.1977, accuracy 56.76%\n",
      "Epoch 35, CIFAR-10 Batch 2:  \tBatch: loss 1.1685 and accuracy 59.03%\t\tValid: loss 1.1869, accuracy 56.72%\n",
      "Epoch 35, CIFAR-10 Batch 3:  \tBatch: loss 1.1596 and accuracy 58.54%\t\tValid: loss 1.2047, accuracy 56.78%\n",
      "Epoch 35, CIFAR-10 Batch 4:  \tBatch: loss 1.1305 and accuracy 61.26%\t\tValid: loss 1.2004, accuracy 56.34%\n",
      "Epoch 35, CIFAR-10 Batch 5:  \tBatch: loss 1.1462 and accuracy 58.66%\t\tValid: loss 1.1937, accuracy 56.46%\n",
      "Epoch 36, CIFAR-10 Batch 1:  \tBatch: loss 1.1301 and accuracy 61.76%\t\tValid: loss 1.1901, accuracy 56.92%\n",
      "Epoch 36, CIFAR-10 Batch 2:  \tBatch: loss 1.1390 and accuracy 60.15%\t\tValid: loss 1.1957, accuracy 56.18%\n",
      "Epoch 36, CIFAR-10 Batch 3:  \tBatch: loss 1.0864 and accuracy 60.40%\t\tValid: loss 1.1789, accuracy 57.18%\n",
      "Epoch 36, CIFAR-10 Batch 4:  \tBatch: loss 1.0999 and accuracy 62.00%\t\tValid: loss 1.1649, accuracy 57.58%\n",
      "Epoch 36, CIFAR-10 Batch 5:  \tBatch: loss 1.0770 and accuracy 63.37%\t\tValid: loss 1.1672, accuracy 57.34%\n",
      "Epoch 37, CIFAR-10 Batch 1:  \tBatch: loss 1.1157 and accuracy 61.63%\t\tValid: loss 1.1816, accuracy 57.28%\n",
      "Epoch 37, CIFAR-10 Batch 2:  \tBatch: loss 1.1257 and accuracy 61.01%\t\tValid: loss 1.1830, accuracy 57.28%\n",
      "Epoch 37, CIFAR-10 Batch 3:  \tBatch: loss 1.1105 and accuracy 60.27%\t\tValid: loss 1.1846, accuracy 57.36%\n",
      "Epoch 37, CIFAR-10 Batch 4:  \tBatch: loss 1.0881 and accuracy 59.78%\t\tValid: loss 1.1663, accuracy 57.84%\n",
      "Epoch 37, CIFAR-10 Batch 5:  \tBatch: loss 1.0995 and accuracy 62.62%\t\tValid: loss 1.1893, accuracy 56.56%\n",
      "Epoch 38, CIFAR-10 Batch 1:  \tBatch: loss 1.0839 and accuracy 63.37%\t\tValid: loss 1.1762, accuracy 57.44%\n",
      "Epoch 38, CIFAR-10 Batch 2:  \tBatch: loss 1.1221 and accuracy 61.39%\t\tValid: loss 1.1956, accuracy 56.16%\n",
      "Epoch 38, CIFAR-10 Batch 3:  \tBatch: loss 1.0821 and accuracy 59.78%\t\tValid: loss 1.1947, accuracy 57.64%\n",
      "Epoch 38, CIFAR-10 Batch 4:  \tBatch: loss 1.0596 and accuracy 63.37%\t\tValid: loss 1.1761, accuracy 57.08%\n",
      "Epoch 38, CIFAR-10 Batch 5:  \tBatch: loss 1.0581 and accuracy 63.74%\t\tValid: loss 1.1741, accuracy 57.02%\n",
      "Epoch 39, CIFAR-10 Batch 1:  \tBatch: loss 1.0899 and accuracy 63.37%\t\tValid: loss 1.1775, accuracy 57.60%\n",
      "Epoch 39, CIFAR-10 Batch 2:  \tBatch: loss 1.0991 and accuracy 60.77%\t\tValid: loss 1.1645, accuracy 57.54%\n",
      "Epoch 39, CIFAR-10 Batch 3:  \tBatch: loss 1.0594 and accuracy 62.13%\t\tValid: loss 1.1884, accuracy 57.42%\n",
      "Epoch 39, CIFAR-10 Batch 4:  \tBatch: loss 1.1073 and accuracy 63.37%\t\tValid: loss 1.2034, accuracy 57.40%\n",
      "Epoch 39, CIFAR-10 Batch 5:  \tBatch: loss 1.0642 and accuracy 63.61%\t\tValid: loss 1.1736, accuracy 57.02%\n",
      "Epoch 40, CIFAR-10 Batch 1:  \tBatch: loss 1.0972 and accuracy 61.88%\t\tValid: loss 1.1849, accuracy 57.68%\n",
      "Epoch 40, CIFAR-10 Batch 2:  \tBatch: loss 1.0727 and accuracy 61.76%\t\tValid: loss 1.1630, accuracy 58.16%\n",
      "Epoch 40, CIFAR-10 Batch 3:  \tBatch: loss 1.0817 and accuracy 62.13%\t\tValid: loss 1.2128, accuracy 57.48%\n",
      "Epoch 40, CIFAR-10 Batch 4:  \tBatch: loss 1.0626 and accuracy 63.00%\t\tValid: loss 1.1582, accuracy 57.88%\n",
      "Epoch 40, CIFAR-10 Batch 5:  \tBatch: loss 1.0822 and accuracy 61.88%\t\tValid: loss 1.1756, accuracy 57.86%\n",
      "Epoch 41, CIFAR-10 Batch 1:  \tBatch: loss 1.0375 and accuracy 62.62%\t\tValid: loss 1.1554, accuracy 58.56%\n",
      "Epoch 41, CIFAR-10 Batch 2:  \tBatch: loss 1.0569 and accuracy 61.76%\t\tValid: loss 1.1608, accuracy 58.40%\n",
      "Epoch 41, CIFAR-10 Batch 3:  \tBatch: loss 1.0601 and accuracy 62.75%\t\tValid: loss 1.1958, accuracy 58.74%\n",
      "Epoch 41, CIFAR-10 Batch 4:  \tBatch: loss 1.0140 and accuracy 63.99%\t\tValid: loss 1.1524, accuracy 58.74%\n",
      "Epoch 41, CIFAR-10 Batch 5:  \tBatch: loss 1.0086 and accuracy 63.49%\t\tValid: loss 1.1626, accuracy 58.32%\n",
      "Epoch 42, CIFAR-10 Batch 1:  \tBatch: loss 1.0121 and accuracy 63.61%\t\tValid: loss 1.1715, accuracy 58.98%\n",
      "Epoch 42, CIFAR-10 Batch 2:  \tBatch: loss 1.0996 and accuracy 61.39%\t\tValid: loss 1.1722, accuracy 58.18%\n",
      "Epoch 42, CIFAR-10 Batch 3:  \tBatch: loss 0.9974 and accuracy 63.61%\t\tValid: loss 1.1783, accuracy 58.72%\n",
      "Epoch 42, CIFAR-10 Batch 4:  \tBatch: loss 1.0330 and accuracy 63.99%\t\tValid: loss 1.1404, accuracy 58.94%\n",
      "Epoch 42, CIFAR-10 Batch 5:  \tBatch: loss 0.9686 and accuracy 66.71%\t\tValid: loss 1.1441, accuracy 59.20%\n",
      "Epoch 43, CIFAR-10 Batch 1:  \tBatch: loss 0.9514 and accuracy 68.07%\t\tValid: loss 1.1484, accuracy 58.98%\n",
      "Epoch 43, CIFAR-10 Batch 2:  \tBatch: loss 1.0473 and accuracy 62.50%\t\tValid: loss 1.1837, accuracy 57.90%\n",
      "Epoch 43, CIFAR-10 Batch 3:  \tBatch: loss 0.9883 and accuracy 64.36%\t\tValid: loss 1.1880, accuracy 58.32%\n",
      "Epoch 43, CIFAR-10 Batch 4:  \tBatch: loss 1.0144 and accuracy 65.10%\t\tValid: loss 1.1496, accuracy 58.72%\n",
      "Epoch 43, CIFAR-10 Batch 5:  \tBatch: loss 1.0335 and accuracy 64.60%\t\tValid: loss 1.1534, accuracy 59.40%\n",
      "Epoch 44, CIFAR-10 Batch 1:  \tBatch: loss 1.0268 and accuracy 63.37%\t\tValid: loss 1.1571, accuracy 59.08%\n",
      "Epoch 44, CIFAR-10 Batch 2:  \tBatch: loss 1.1049 and accuracy 62.50%\t\tValid: loss 1.1840, accuracy 58.44%\n",
      "Epoch 44, CIFAR-10 Batch 3:  \tBatch: loss 1.0057 and accuracy 63.99%\t\tValid: loss 1.1882, accuracy 57.78%\n",
      "Epoch 44, CIFAR-10 Batch 4:  \tBatch: loss 1.0567 and accuracy 64.11%\t\tValid: loss 1.1903, accuracy 58.10%\n",
      "Epoch 44, CIFAR-10 Batch 5:  \tBatch: loss 1.0447 and accuracy 64.73%\t\tValid: loss 1.1627, accuracy 58.72%\n",
      "Epoch 45, CIFAR-10 Batch 1:  \tBatch: loss 1.0246 and accuracy 66.09%\t\tValid: loss 1.1452, accuracy 59.10%\n",
      "Epoch 45, CIFAR-10 Batch 2:  \tBatch: loss 1.0115 and accuracy 65.10%\t\tValid: loss 1.1604, accuracy 59.22%\n",
      "Epoch 45, CIFAR-10 Batch 3:  \tBatch: loss 1.0183 and accuracy 63.49%\t\tValid: loss 1.1745, accuracy 58.96%\n",
      "Epoch 45, CIFAR-10 Batch 4:  \tBatch: loss 0.9885 and accuracy 64.48%\t\tValid: loss 1.1520, accuracy 58.90%\n",
      "Epoch 45, CIFAR-10 Batch 5:  \tBatch: loss 0.9787 and accuracy 65.59%\t\tValid: loss 1.1420, accuracy 59.86%\n",
      "Epoch 46, CIFAR-10 Batch 1:  \tBatch: loss 0.9926 and accuracy 63.99%\t\tValid: loss 1.1453, accuracy 59.58%\n",
      "Epoch 46, CIFAR-10 Batch 2:  \tBatch: loss 1.0050 and accuracy 64.11%\t\tValid: loss 1.1664, accuracy 58.94%\n",
      "Epoch 46, CIFAR-10 Batch 3:  \tBatch: loss 0.9632 and accuracy 66.71%\t\tValid: loss 1.1796, accuracy 58.86%\n",
      "Epoch 46, CIFAR-10 Batch 4:  \tBatch: loss 1.0615 and accuracy 63.74%\t\tValid: loss 1.1656, accuracy 58.30%\n",
      "Epoch 46, CIFAR-10 Batch 5:  \tBatch: loss 0.9918 and accuracy 63.99%\t\tValid: loss 1.1334, accuracy 60.04%\n",
      "Epoch 47, CIFAR-10 Batch 1:  \tBatch: loss 0.9693 and accuracy 66.71%\t\tValid: loss 1.1333, accuracy 59.98%\n",
      "Epoch 47, CIFAR-10 Batch 2:  \tBatch: loss 0.9891 and accuracy 64.11%\t\tValid: loss 1.1531, accuracy 59.86%\n",
      "Epoch 47, CIFAR-10 Batch 3:  \tBatch: loss 0.9974 and accuracy 63.00%\t\tValid: loss 1.1936, accuracy 57.84%\n",
      "Epoch 47, CIFAR-10 Batch 4:  \tBatch: loss 0.9842 and accuracy 64.98%\t\tValid: loss 1.1715, accuracy 58.36%\n",
      "Epoch 47, CIFAR-10 Batch 5:  \tBatch: loss 0.9553 and accuracy 65.59%\t\tValid: loss 1.1368, accuracy 59.82%\n",
      "Epoch 48, CIFAR-10 Batch 1:  \tBatch: loss 0.9387 and accuracy 67.82%\t\tValid: loss 1.1363, accuracy 59.94%\n",
      "Epoch 48, CIFAR-10 Batch 2:  \tBatch: loss 0.9746 and accuracy 64.98%\t\tValid: loss 1.1521, accuracy 59.44%\n",
      "Epoch 48, CIFAR-10 Batch 3:  \tBatch: loss 0.9816 and accuracy 65.59%\t\tValid: loss 1.1760, accuracy 59.14%\n",
      "Epoch 48, CIFAR-10 Batch 4:  \tBatch: loss 0.9880 and accuracy 66.34%\t\tValid: loss 1.1463, accuracy 58.58%\n",
      "Epoch 48, CIFAR-10 Batch 5:  \tBatch: loss 0.9409 and accuracy 68.44%\t\tValid: loss 1.1261, accuracy 59.90%\n",
      "Epoch 49, CIFAR-10 Batch 1:  \tBatch: loss 0.9393 and accuracy 68.56%\t\tValid: loss 1.1288, accuracy 60.42%\n",
      "Epoch 49, CIFAR-10 Batch 2:  \tBatch: loss 0.9355 and accuracy 67.45%\t\tValid: loss 1.1406, accuracy 60.24%\n",
      "Epoch 49, CIFAR-10 Batch 3:  \tBatch: loss 0.9025 and accuracy 66.34%\t\tValid: loss 1.1478, accuracy 59.96%\n",
      "Epoch 49, CIFAR-10 Batch 4:  \tBatch: loss 0.9652 and accuracy 64.36%\t\tValid: loss 1.1417, accuracy 59.48%\n",
      "Epoch 49, CIFAR-10 Batch 5:  \tBatch: loss 0.9043 and accuracy 66.21%\t\tValid: loss 1.1309, accuracy 60.06%\n",
      "Epoch 50, CIFAR-10 Batch 1:  \tBatch: loss 0.9282 and accuracy 69.43%\t\tValid: loss 1.1295, accuracy 60.64%\n",
      "Epoch 50, CIFAR-10 Batch 2:  \tBatch: loss 0.9615 and accuracy 67.08%\t\tValid: loss 1.1387, accuracy 60.34%\n",
      "Epoch 50, CIFAR-10 Batch 3:  \tBatch: loss 0.9304 and accuracy 67.82%\t\tValid: loss 1.1485, accuracy 59.58%\n",
      "Epoch 50, CIFAR-10 Batch 4:  \tBatch: loss 0.9188 and accuracy 69.68%\t\tValid: loss 1.1477, accuracy 59.22%\n",
      "Epoch 50, CIFAR-10 Batch 5:  \tBatch: loss 0.9563 and accuracy 66.46%\t\tValid: loss 1.1394, accuracy 59.86%\n",
      "Epoch 51, CIFAR-10 Batch 1:  \tBatch: loss 0.9021 and accuracy 68.69%\t\tValid: loss 1.1282, accuracy 61.00%\n",
      "Epoch 51, CIFAR-10 Batch 2:  \tBatch: loss 0.9693 and accuracy 66.46%\t\tValid: loss 1.1345, accuracy 60.22%\n",
      "Epoch 51, CIFAR-10 Batch 3:  \tBatch: loss 0.8845 and accuracy 67.20%\t\tValid: loss 1.1435, accuracy 59.68%\n",
      "Epoch 51, CIFAR-10 Batch 4:  \tBatch: loss 0.9159 and accuracy 66.71%\t\tValid: loss 1.1415, accuracy 60.12%\n",
      "Epoch 51, CIFAR-10 Batch 5:  \tBatch: loss 0.9436 and accuracy 65.72%\t\tValid: loss 1.1277, accuracy 60.44%\n",
      "Epoch 52, CIFAR-10 Batch 1:  \tBatch: loss 0.9495 and accuracy 66.46%\t\tValid: loss 1.1301, accuracy 60.30%\n",
      "Epoch 52, CIFAR-10 Batch 2:  \tBatch: loss 0.8871 and accuracy 66.58%\t\tValid: loss 1.1235, accuracy 60.52%\n",
      "Epoch 52, CIFAR-10 Batch 3:  \tBatch: loss 0.9230 and accuracy 66.21%\t\tValid: loss 1.1543, accuracy 59.70%\n",
      "Epoch 52, CIFAR-10 Batch 4:  \tBatch: loss 0.9507 and accuracy 66.09%\t\tValid: loss 1.1806, accuracy 58.76%\n",
      "Epoch 52, CIFAR-10 Batch 5:  \tBatch: loss 0.9192 and accuracy 67.45%\t\tValid: loss 1.1343, accuracy 60.16%\n",
      "Epoch 53, CIFAR-10 Batch 1:  \tBatch: loss 0.8909 and accuracy 68.69%\t\tValid: loss 1.1343, accuracy 60.86%\n",
      "Epoch 53, CIFAR-10 Batch 2:  \tBatch: loss 0.9335 and accuracy 67.57%\t\tValid: loss 1.1225, accuracy 60.32%\n",
      "Epoch 53, CIFAR-10 Batch 3:  \tBatch: loss 0.8634 and accuracy 69.80%\t\tValid: loss 1.1285, accuracy 60.78%\n",
      "Epoch 53, CIFAR-10 Batch 4:  \tBatch: loss 0.9166 and accuracy 68.44%\t\tValid: loss 1.1656, accuracy 59.82%\n",
      "Epoch 53, CIFAR-10 Batch 5:  \tBatch: loss 0.8938 and accuracy 68.07%\t\tValid: loss 1.1455, accuracy 59.76%\n",
      "Epoch 54, CIFAR-10 Batch 1:  \tBatch: loss 0.8779 and accuracy 70.42%\t\tValid: loss 1.1154, accuracy 60.78%\n",
      "Epoch 54, CIFAR-10 Batch 2:  \tBatch: loss 0.9280 and accuracy 66.83%\t\tValid: loss 1.1073, accuracy 60.72%\n",
      "Epoch 54, CIFAR-10 Batch 3:  \tBatch: loss 0.9102 and accuracy 67.70%\t\tValid: loss 1.1187, accuracy 61.18%\n",
      "Epoch 54, CIFAR-10 Batch 4:  \tBatch: loss 0.9283 and accuracy 67.70%\t\tValid: loss 1.1624, accuracy 59.96%\n",
      "Epoch 54, CIFAR-10 Batch 5:  \tBatch: loss 0.8836 and accuracy 69.06%\t\tValid: loss 1.1314, accuracy 60.06%\n",
      "Epoch 55, CIFAR-10 Batch 1:  \tBatch: loss 0.8832 and accuracy 68.94%\t\tValid: loss 1.1214, accuracy 60.60%\n",
      "Epoch 55, CIFAR-10 Batch 2:  \tBatch: loss 0.8618 and accuracy 68.56%\t\tValid: loss 1.1286, accuracy 60.24%\n",
      "Epoch 55, CIFAR-10 Batch 3:  \tBatch: loss 0.8353 and accuracy 69.06%\t\tValid: loss 1.1396, accuracy 60.44%\n",
      "Epoch 55, CIFAR-10 Batch 4:  \tBatch: loss 0.9155 and accuracy 69.55%\t\tValid: loss 1.1457, accuracy 60.14%\n",
      "Epoch 55, CIFAR-10 Batch 5:  \tBatch: loss 0.8495 and accuracy 71.91%\t\tValid: loss 1.1078, accuracy 61.28%\n",
      "Epoch 56, CIFAR-10 Batch 1:  \tBatch: loss 0.8521 and accuracy 71.04%\t\tValid: loss 1.1046, accuracy 61.62%\n",
      "Epoch 56, CIFAR-10 Batch 2:  \tBatch: loss 0.8487 and accuracy 69.68%\t\tValid: loss 1.1119, accuracy 61.60%\n",
      "Epoch 56, CIFAR-10 Batch 3:  \tBatch: loss 0.8291 and accuracy 69.31%\t\tValid: loss 1.1534, accuracy 60.70%\n",
      "Epoch 56, CIFAR-10 Batch 4:  \tBatch: loss 0.8235 and accuracy 71.78%\t\tValid: loss 1.1407, accuracy 60.68%\n",
      "Epoch 56, CIFAR-10 Batch 5:  \tBatch: loss 0.8432 and accuracy 70.54%\t\tValid: loss 1.1178, accuracy 61.78%\n",
      "Epoch 57, CIFAR-10 Batch 1:  \tBatch: loss 0.8242 and accuracy 72.52%\t\tValid: loss 1.1154, accuracy 61.60%\n",
      "Epoch 57, CIFAR-10 Batch 2:  \tBatch: loss 0.8631 and accuracy 68.81%\t\tValid: loss 1.1263, accuracy 61.28%\n",
      "Epoch 57, CIFAR-10 Batch 3:  \tBatch: loss 0.7989 and accuracy 71.53%\t\tValid: loss 1.1396, accuracy 61.14%\n",
      "Epoch 57, CIFAR-10 Batch 4:  \tBatch: loss 0.8511 and accuracy 69.68%\t\tValid: loss 1.1543, accuracy 60.28%\n",
      "Epoch 57, CIFAR-10 Batch 5:  \tBatch: loss 0.7820 and accuracy 72.90%\t\tValid: loss 1.1196, accuracy 61.42%\n",
      "Epoch 58, CIFAR-10 Batch 1:  \tBatch: loss 0.7847 and accuracy 73.39%\t\tValid: loss 1.1170, accuracy 61.32%\n",
      "Epoch 58, CIFAR-10 Batch 2:  \tBatch: loss 0.8462 and accuracy 71.16%\t\tValid: loss 1.1300, accuracy 61.52%\n",
      "Epoch 58, CIFAR-10 Batch 3:  \tBatch: loss 0.8493 and accuracy 69.55%\t\tValid: loss 1.1289, accuracy 62.04%\n",
      "Epoch 58, CIFAR-10 Batch 4:  \tBatch: loss 0.8218 and accuracy 71.16%\t\tValid: loss 1.1509, accuracy 60.10%\n",
      "Epoch 58, CIFAR-10 Batch 5:  \tBatch: loss 0.7905 and accuracy 72.03%\t\tValid: loss 1.1180, accuracy 61.58%\n",
      "Epoch 59, CIFAR-10 Batch 1:  \tBatch: loss 0.7891 and accuracy 73.14%\t\tValid: loss 1.1252, accuracy 61.74%\n",
      "Epoch 59, CIFAR-10 Batch 2:  \tBatch: loss 0.8060 and accuracy 73.02%\t\tValid: loss 1.1202, accuracy 62.44%\n",
      "Epoch 59, CIFAR-10 Batch 3:  \tBatch: loss 0.8751 and accuracy 71.29%\t\tValid: loss 1.1888, accuracy 60.70%\n",
      "Epoch 59, CIFAR-10 Batch 4:  \tBatch: loss 0.8149 and accuracy 71.04%\t\tValid: loss 1.1752, accuracy 60.34%\n",
      "Epoch 59, CIFAR-10 Batch 5:  \tBatch: loss 0.7943 and accuracy 72.77%\t\tValid: loss 1.1246, accuracy 61.20%\n",
      "Epoch 60, CIFAR-10 Batch 1:  \tBatch: loss 0.7936 and accuracy 71.66%\t\tValid: loss 1.1227, accuracy 61.54%\n",
      "Epoch 60, CIFAR-10 Batch 2:  \tBatch: loss 0.8176 and accuracy 72.52%\t\tValid: loss 1.1278, accuracy 61.22%\n",
      "Epoch 60, CIFAR-10 Batch 3:  \tBatch: loss 0.7641 and accuracy 72.52%\t\tValid: loss 1.1272, accuracy 62.40%\n",
      "Epoch 60, CIFAR-10 Batch 4:  \tBatch: loss 0.7993 and accuracy 73.51%\t\tValid: loss 1.1526, accuracy 60.44%\n",
      "Epoch 60, CIFAR-10 Batch 5:  \tBatch: loss 0.7784 and accuracy 72.90%\t\tValid: loss 1.1391, accuracy 61.80%\n",
      "Epoch 61, CIFAR-10 Batch 1:  \tBatch: loss 0.7535 and accuracy 73.51%\t\tValid: loss 1.1193, accuracy 61.78%\n",
      "Epoch 61, CIFAR-10 Batch 2:  \tBatch: loss 0.7871 and accuracy 73.14%\t\tValid: loss 1.1407, accuracy 61.04%\n",
      "Epoch 61, CIFAR-10 Batch 3:  \tBatch: loss 0.7467 and accuracy 73.89%\t\tValid: loss 1.1288, accuracy 61.76%\n",
      "Epoch 61, CIFAR-10 Batch 4:  \tBatch: loss 0.7890 and accuracy 71.41%\t\tValid: loss 1.1690, accuracy 60.72%\n",
      "Epoch 61, CIFAR-10 Batch 5:  \tBatch: loss 0.7559 and accuracy 73.64%\t\tValid: loss 1.1452, accuracy 60.76%\n",
      "Epoch 62, CIFAR-10 Batch 1:  \tBatch: loss 0.7515 and accuracy 72.90%\t\tValid: loss 1.1372, accuracy 61.80%\n",
      "Epoch 62, CIFAR-10 Batch 2:  \tBatch: loss 0.7753 and accuracy 71.78%\t\tValid: loss 1.1329, accuracy 61.60%\n",
      "Epoch 62, CIFAR-10 Batch 3:  \tBatch: loss 0.8006 and accuracy 68.32%\t\tValid: loss 1.1247, accuracy 62.16%\n",
      "Epoch 62, CIFAR-10 Batch 4:  \tBatch: loss 0.8293 and accuracy 70.42%\t\tValid: loss 1.1421, accuracy 60.74%\n",
      "Epoch 62, CIFAR-10 Batch 5:  \tBatch: loss 0.7559 and accuracy 71.91%\t\tValid: loss 1.1503, accuracy 61.42%\n",
      "Epoch 63, CIFAR-10 Batch 1:  \tBatch: loss 0.7467 and accuracy 75.25%\t\tValid: loss 1.1534, accuracy 61.70%\n",
      "Epoch 63, CIFAR-10 Batch 2:  \tBatch: loss 0.7543 and accuracy 73.89%\t\tValid: loss 1.1307, accuracy 62.42%\n",
      "Epoch 63, CIFAR-10 Batch 3:  \tBatch: loss 0.7807 and accuracy 71.16%\t\tValid: loss 1.1616, accuracy 61.00%\n",
      "Epoch 63, CIFAR-10 Batch 4:  \tBatch: loss 0.7903 and accuracy 73.64%\t\tValid: loss 1.1508, accuracy 61.50%\n",
      "Epoch 63, CIFAR-10 Batch 5:  \tBatch: loss 0.7298 and accuracy 73.76%\t\tValid: loss 1.1508, accuracy 61.44%\n",
      "Epoch 64, CIFAR-10 Batch 1:  \tBatch: loss 0.7574 and accuracy 72.90%\t\tValid: loss 1.1641, accuracy 61.28%\n",
      "Epoch 64, CIFAR-10 Batch 2:  \tBatch: loss 0.7428 and accuracy 72.65%\t\tValid: loss 1.1436, accuracy 62.28%\n",
      "Epoch 64, CIFAR-10 Batch 3:  \tBatch: loss 0.7241 and accuracy 72.65%\t\tValid: loss 1.1366, accuracy 62.32%\n",
      "Epoch 64, CIFAR-10 Batch 4:  \tBatch: loss 0.7684 and accuracy 73.39%\t\tValid: loss 1.1643, accuracy 61.24%\n",
      "Epoch 64, CIFAR-10 Batch 5:  \tBatch: loss 0.7391 and accuracy 74.75%\t\tValid: loss 1.1481, accuracy 62.28%\n",
      "Epoch 65, CIFAR-10 Batch 1:  \tBatch: loss 0.7776 and accuracy 71.41%\t\tValid: loss 1.1725, accuracy 61.54%\n",
      "Epoch 65, CIFAR-10 Batch 2:  \tBatch: loss 0.7928 and accuracy 72.03%\t\tValid: loss 1.1504, accuracy 61.98%\n",
      "Epoch 65, CIFAR-10 Batch 3:  \tBatch: loss 0.7273 and accuracy 73.76%\t\tValid: loss 1.1460, accuracy 62.06%\n",
      "Epoch 65, CIFAR-10 Batch 4:  \tBatch: loss 0.7423 and accuracy 74.63%\t\tValid: loss 1.1419, accuracy 62.48%\n",
      "Epoch 65, CIFAR-10 Batch 5:  \tBatch: loss 0.7251 and accuracy 75.25%\t\tValid: loss 1.1725, accuracy 61.52%\n",
      "Epoch 66, CIFAR-10 Batch 1:  \tBatch: loss 0.7603 and accuracy 73.27%\t\tValid: loss 1.2003, accuracy 60.26%\n",
      "Epoch 66, CIFAR-10 Batch 2:  \tBatch: loss 0.8013 and accuracy 71.29%\t\tValid: loss 1.1664, accuracy 60.96%\n",
      "Epoch 66, CIFAR-10 Batch 3:  \tBatch: loss 0.7887 and accuracy 71.29%\t\tValid: loss 1.1769, accuracy 61.16%\n",
      "Epoch 66, CIFAR-10 Batch 4:  \tBatch: loss 0.7831 and accuracy 72.90%\t\tValid: loss 1.1444, accuracy 61.72%\n",
      "Epoch 66, CIFAR-10 Batch 5:  \tBatch: loss 0.7329 and accuracy 74.13%\t\tValid: loss 1.1737, accuracy 60.74%\n",
      "Epoch 67, CIFAR-10 Batch 1:  \tBatch: loss 0.7145 and accuracy 75.12%\t\tValid: loss 1.1593, accuracy 61.20%\n",
      "Epoch 67, CIFAR-10 Batch 2:  \tBatch: loss 0.7429 and accuracy 73.64%\t\tValid: loss 1.1451, accuracy 62.42%\n",
      "Epoch 67, CIFAR-10 Batch 3:  \tBatch: loss 0.7700 and accuracy 70.30%\t\tValid: loss 1.1704, accuracy 61.30%\n",
      "Epoch 67, CIFAR-10 Batch 4:  \tBatch: loss 0.7256 and accuracy 74.63%\t\tValid: loss 1.1507, accuracy 62.22%\n",
      "Epoch 67, CIFAR-10 Batch 5:  \tBatch: loss 0.7293 and accuracy 74.26%\t\tValid: loss 1.1658, accuracy 61.52%\n",
      "Epoch 68, CIFAR-10 Batch 1:  \tBatch: loss 0.7099 and accuracy 75.62%\t\tValid: loss 1.1555, accuracy 61.62%\n",
      "Epoch 68, CIFAR-10 Batch 2:  \tBatch: loss 0.7584 and accuracy 73.76%\t\tValid: loss 1.1385, accuracy 61.68%\n",
      "Epoch 68, CIFAR-10 Batch 3:  \tBatch: loss 0.7151 and accuracy 73.64%\t\tValid: loss 1.1510, accuracy 62.10%\n",
      "Epoch 68, CIFAR-10 Batch 4:  \tBatch: loss 0.7240 and accuracy 76.24%\t\tValid: loss 1.1545, accuracy 61.46%\n",
      "Epoch 68, CIFAR-10 Batch 5:  \tBatch: loss 0.7470 and accuracy 74.63%\t\tValid: loss 1.1988, accuracy 61.12%\n",
      "Epoch 69, CIFAR-10 Batch 1:  \tBatch: loss 0.6910 and accuracy 78.09%\t\tValid: loss 1.1422, accuracy 61.86%\n",
      "Epoch 69, CIFAR-10 Batch 2:  \tBatch: loss 0.8181 and accuracy 70.67%\t\tValid: loss 1.1539, accuracy 61.96%\n",
      "Epoch 69, CIFAR-10 Batch 3:  \tBatch: loss 0.7848 and accuracy 73.02%\t\tValid: loss 1.1677, accuracy 61.78%\n",
      "Epoch 69, CIFAR-10 Batch 4:  \tBatch: loss 0.7447 and accuracy 73.14%\t\tValid: loss 1.1505, accuracy 61.80%\n",
      "Epoch 69, CIFAR-10 Batch 5:  \tBatch: loss 0.7044 and accuracy 76.86%\t\tValid: loss 1.1526, accuracy 61.82%\n",
      "Epoch 70, CIFAR-10 Batch 1:  \tBatch: loss 0.7449 and accuracy 73.51%\t\tValid: loss 1.1503, accuracy 61.26%\n",
      "Epoch 70, CIFAR-10 Batch 2:  \tBatch: loss 0.7796 and accuracy 72.28%\t\tValid: loss 1.1448, accuracy 62.02%\n",
      "Epoch 70, CIFAR-10 Batch 3:  \tBatch: loss 0.7094 and accuracy 75.25%\t\tValid: loss 1.1413, accuracy 62.24%\n",
      "Epoch 70, CIFAR-10 Batch 4:  \tBatch: loss 0.7126 and accuracy 75.50%\t\tValid: loss 1.1398, accuracy 62.16%\n",
      "Epoch 70, CIFAR-10 Batch 5:  \tBatch: loss 0.6536 and accuracy 77.10%\t\tValid: loss 1.1409, accuracy 62.40%\n",
      "Epoch 71, CIFAR-10 Batch 1:  \tBatch: loss 0.7082 and accuracy 76.49%\t\tValid: loss 1.1670, accuracy 61.74%\n",
      "Epoch 71, CIFAR-10 Batch 2:  \tBatch: loss 0.7469 and accuracy 73.51%\t\tValid: loss 1.1493, accuracy 62.10%\n",
      "Epoch 71, CIFAR-10 Batch 3:  \tBatch: loss 0.7200 and accuracy 73.64%\t\tValid: loss 1.1609, accuracy 62.38%\n",
      "Epoch 71, CIFAR-10 Batch 4:  \tBatch: loss 0.6956 and accuracy 75.50%\t\tValid: loss 1.1410, accuracy 61.70%\n",
      "Epoch 71, CIFAR-10 Batch 5:  \tBatch: loss 0.7546 and accuracy 74.13%\t\tValid: loss 1.1675, accuracy 61.44%\n",
      "Epoch 72, CIFAR-10 Batch 1:  \tBatch: loss 0.6850 and accuracy 77.85%\t\tValid: loss 1.1526, accuracy 61.80%\n",
      "Epoch 72, CIFAR-10 Batch 2:  \tBatch: loss 0.7867 and accuracy 72.28%\t\tValid: loss 1.1926, accuracy 60.68%\n",
      "Epoch 72, CIFAR-10 Batch 3:  \tBatch: loss 0.7744 and accuracy 71.04%\t\tValid: loss 1.1581, accuracy 60.94%\n",
      "Epoch 72, CIFAR-10 Batch 4:  \tBatch: loss 0.7332 and accuracy 75.37%\t\tValid: loss 1.1642, accuracy 60.74%\n",
      "Epoch 72, CIFAR-10 Batch 5:  \tBatch: loss 0.7074 and accuracy 75.99%\t\tValid: loss 1.1494, accuracy 61.14%\n",
      "Epoch 73, CIFAR-10 Batch 1:  \tBatch: loss 0.7064 and accuracy 75.00%\t\tValid: loss 1.1321, accuracy 62.30%\n",
      "Epoch 73, CIFAR-10 Batch 2:  \tBatch: loss 0.7869 and accuracy 72.77%\t\tValid: loss 1.2054, accuracy 61.02%\n",
      "Epoch 73, CIFAR-10 Batch 3:  \tBatch: loss 0.7152 and accuracy 75.25%\t\tValid: loss 1.1842, accuracy 60.76%\n",
      "Epoch 73, CIFAR-10 Batch 4:  \tBatch: loss 0.6903 and accuracy 75.12%\t\tValid: loss 1.1527, accuracy 61.40%\n",
      "Epoch 73, CIFAR-10 Batch 5:  \tBatch: loss 0.7274 and accuracy 73.76%\t\tValid: loss 1.1585, accuracy 61.62%\n",
      "Epoch 74, CIFAR-10 Batch 1:  \tBatch: loss 0.6962 and accuracy 77.35%\t\tValid: loss 1.1518, accuracy 61.36%\n",
      "Epoch 74, CIFAR-10 Batch 2:  \tBatch: loss 0.7220 and accuracy 75.74%\t\tValid: loss 1.1579, accuracy 62.10%\n",
      "Epoch 74, CIFAR-10 Batch 3:  \tBatch: loss 0.7128 and accuracy 72.65%\t\tValid: loss 1.1791, accuracy 61.62%\n",
      "Epoch 74, CIFAR-10 Batch 4:  \tBatch: loss 0.6984 and accuracy 74.63%\t\tValid: loss 1.1532, accuracy 62.20%\n",
      "Epoch 74, CIFAR-10 Batch 5:  \tBatch: loss 0.6486 and accuracy 77.85%\t\tValid: loss 1.1602, accuracy 61.80%\n",
      "Epoch 75, CIFAR-10 Batch 1:  \tBatch: loss 0.6720 and accuracy 75.37%\t\tValid: loss 1.1498, accuracy 61.84%\n",
      "Epoch 75, CIFAR-10 Batch 2:  \tBatch: loss 0.7335 and accuracy 75.12%\t\tValid: loss 1.1683, accuracy 61.86%\n",
      "Epoch 75, CIFAR-10 Batch 3:  \tBatch: loss 0.6796 and accuracy 75.87%\t\tValid: loss 1.2136, accuracy 60.82%\n",
      "Epoch 75, CIFAR-10 Batch 4:  \tBatch: loss 0.7063 and accuracy 75.25%\t\tValid: loss 1.1672, accuracy 61.80%\n",
      "Epoch 75, CIFAR-10 Batch 5:  \tBatch: loss 0.6648 and accuracy 76.24%\t\tValid: loss 1.1590, accuracy 62.08%\n",
      "Epoch 76, CIFAR-10 Batch 1:  \tBatch: loss 0.6201 and accuracy 77.48%\t\tValid: loss 1.1562, accuracy 62.48%\n",
      "Epoch 76, CIFAR-10 Batch 2:  \tBatch: loss 0.7107 and accuracy 74.13%\t\tValid: loss 1.1513, accuracy 62.10%\n",
      "Epoch 76, CIFAR-10 Batch 3:  \tBatch: loss 0.6540 and accuracy 75.99%\t\tValid: loss 1.1843, accuracy 61.58%\n",
      "Epoch 76, CIFAR-10 Batch 4:  \tBatch: loss 0.7008 and accuracy 76.61%\t\tValid: loss 1.1602, accuracy 62.70%\n",
      "Epoch 76, CIFAR-10 Batch 5:  \tBatch: loss 0.6453 and accuracy 79.08%\t\tValid: loss 1.1659, accuracy 62.34%\n",
      "Epoch 77, CIFAR-10 Batch 1:  \tBatch: loss 0.6168 and accuracy 77.35%\t\tValid: loss 1.1619, accuracy 62.14%\n",
      "Epoch 77, CIFAR-10 Batch 2:  \tBatch: loss 0.6880 and accuracy 77.10%\t\tValid: loss 1.1783, accuracy 62.38%\n",
      "Epoch 77, CIFAR-10 Batch 3:  \tBatch: loss 0.6650 and accuracy 76.11%\t\tValid: loss 1.2020, accuracy 61.80%\n",
      "Epoch 77, CIFAR-10 Batch 4:  \tBatch: loss 0.7087 and accuracy 74.63%\t\tValid: loss 1.1733, accuracy 61.82%\n",
      "Epoch 77, CIFAR-10 Batch 5:  \tBatch: loss 0.6315 and accuracy 78.22%\t\tValid: loss 1.1756, accuracy 61.70%\n",
      "Epoch 78, CIFAR-10 Batch 1:  \tBatch: loss 0.6559 and accuracy 76.11%\t\tValid: loss 1.1540, accuracy 62.04%\n",
      "Epoch 78, CIFAR-10 Batch 2:  \tBatch: loss 0.6814 and accuracy 76.61%\t\tValid: loss 1.1990, accuracy 61.46%\n",
      "Epoch 78, CIFAR-10 Batch 3:  \tBatch: loss 0.6828 and accuracy 76.36%\t\tValid: loss 1.1960, accuracy 61.56%\n",
      "Epoch 78, CIFAR-10 Batch 4:  \tBatch: loss 0.6830 and accuracy 76.49%\t\tValid: loss 1.1785, accuracy 61.76%\n",
      "Epoch 78, CIFAR-10 Batch 5:  \tBatch: loss 0.6433 and accuracy 78.34%\t\tValid: loss 1.1736, accuracy 61.58%\n",
      "Epoch 79, CIFAR-10 Batch 1:  \tBatch: loss 0.7215 and accuracy 74.50%\t\tValid: loss 1.2031, accuracy 61.66%\n",
      "Epoch 79, CIFAR-10 Batch 2:  \tBatch: loss 0.6897 and accuracy 76.49%\t\tValid: loss 1.1586, accuracy 62.22%\n",
      "Epoch 79, CIFAR-10 Batch 3:  \tBatch: loss 0.7019 and accuracy 75.50%\t\tValid: loss 1.2244, accuracy 62.04%\n",
      "Epoch 79, CIFAR-10 Batch 4:  \tBatch: loss 0.6579 and accuracy 78.22%\t\tValid: loss 1.1691, accuracy 62.26%\n",
      "Epoch 79, CIFAR-10 Batch 5:  \tBatch: loss 0.6715 and accuracy 77.23%\t\tValid: loss 1.1766, accuracy 61.28%\n",
      "Epoch 80, CIFAR-10 Batch 1:  \tBatch: loss 0.6431 and accuracy 78.09%\t\tValid: loss 1.1610, accuracy 62.54%\n",
      "Epoch 80, CIFAR-10 Batch 2:  \tBatch: loss 0.6717 and accuracy 75.50%\t\tValid: loss 1.1535, accuracy 62.26%\n",
      "Epoch 80, CIFAR-10 Batch 3:  \tBatch: loss 0.6132 and accuracy 77.72%\t\tValid: loss 1.1807, accuracy 62.36%\n",
      "Epoch 80, CIFAR-10 Batch 4:  \tBatch: loss 0.6586 and accuracy 78.84%\t\tValid: loss 1.1662, accuracy 62.30%\n",
      "Epoch 80, CIFAR-10 Batch 5:  \tBatch: loss 0.6799 and accuracy 76.49%\t\tValid: loss 1.1871, accuracy 61.04%\n",
      "Epoch 81, CIFAR-10 Batch 1:  \tBatch: loss 0.6268 and accuracy 78.09%\t\tValid: loss 1.1716, accuracy 61.96%\n",
      "Epoch 81, CIFAR-10 Batch 2:  \tBatch: loss 0.6435 and accuracy 77.85%\t\tValid: loss 1.1614, accuracy 61.48%\n",
      "Epoch 81, CIFAR-10 Batch 3:  \tBatch: loss 0.6570 and accuracy 76.11%\t\tValid: loss 1.2055, accuracy 61.70%\n",
      "Epoch 81, CIFAR-10 Batch 4:  \tBatch: loss 0.6430 and accuracy 78.22%\t\tValid: loss 1.1838, accuracy 62.30%\n",
      "Epoch 81, CIFAR-10 Batch 5:  \tBatch: loss 0.6646 and accuracy 76.24%\t\tValid: loss 1.2193, accuracy 61.38%\n",
      "Epoch 82, CIFAR-10 Batch 1:  \tBatch: loss 0.5976 and accuracy 77.85%\t\tValid: loss 1.1734, accuracy 62.72%\n",
      "Epoch 82, CIFAR-10 Batch 2:  \tBatch: loss 0.6139 and accuracy 77.72%\t\tValid: loss 1.1524, accuracy 62.86%\n",
      "Epoch 82, CIFAR-10 Batch 3:  \tBatch: loss 0.6443 and accuracy 77.48%\t\tValid: loss 1.2185, accuracy 61.88%\n",
      "Epoch 82, CIFAR-10 Batch 4:  \tBatch: loss 0.6586 and accuracy 76.73%\t\tValid: loss 1.1866, accuracy 62.64%\n",
      "Epoch 82, CIFAR-10 Batch 5:  \tBatch: loss 0.6174 and accuracy 77.85%\t\tValid: loss 1.2179, accuracy 61.22%\n",
      "Epoch 83, CIFAR-10 Batch 1:  \tBatch: loss 0.5993 and accuracy 80.82%\t\tValid: loss 1.1839, accuracy 62.52%\n",
      "Epoch 83, CIFAR-10 Batch 2:  \tBatch: loss 0.6218 and accuracy 78.59%\t\tValid: loss 1.1835, accuracy 62.20%\n",
      "Epoch 83, CIFAR-10 Batch 3:  \tBatch: loss 0.6207 and accuracy 78.59%\t\tValid: loss 1.2048, accuracy 62.06%\n",
      "Epoch 83, CIFAR-10 Batch 4:  \tBatch: loss 0.6488 and accuracy 78.47%\t\tValid: loss 1.2212, accuracy 61.22%\n",
      "Epoch 83, CIFAR-10 Batch 5:  \tBatch: loss 0.5917 and accuracy 80.07%\t\tValid: loss 1.2417, accuracy 61.42%\n",
      "Epoch 84, CIFAR-10 Batch 1:  \tBatch: loss 0.6090 and accuracy 79.08%\t\tValid: loss 1.2034, accuracy 62.12%\n",
      "Epoch 84, CIFAR-10 Batch 2:  \tBatch: loss 0.6759 and accuracy 78.47%\t\tValid: loss 1.1706, accuracy 62.46%\n",
      "Epoch 84, CIFAR-10 Batch 3:  \tBatch: loss 0.6053 and accuracy 80.45%\t\tValid: loss 1.2001, accuracy 62.54%\n",
      "Epoch 84, CIFAR-10 Batch 4:  \tBatch: loss 0.6403 and accuracy 77.60%\t\tValid: loss 1.2010, accuracy 62.10%\n",
      "Epoch 84, CIFAR-10 Batch 5:  \tBatch: loss 0.5495 and accuracy 81.44%\t\tValid: loss 1.1996, accuracy 62.24%\n",
      "Epoch 85, CIFAR-10 Batch 1:  \tBatch: loss 0.5285 and accuracy 81.56%\t\tValid: loss 1.1853, accuracy 62.68%\n",
      "Epoch 85, CIFAR-10 Batch 2:  \tBatch: loss 0.5929 and accuracy 80.94%\t\tValid: loss 1.1733, accuracy 62.78%\n",
      "Epoch 85, CIFAR-10 Batch 3:  \tBatch: loss 0.5993 and accuracy 78.84%\t\tValid: loss 1.2008, accuracy 62.70%\n",
      "Epoch 85, CIFAR-10 Batch 4:  \tBatch: loss 0.6200 and accuracy 79.46%\t\tValid: loss 1.2362, accuracy 62.10%\n",
      "Epoch 85, CIFAR-10 Batch 5:  \tBatch: loss 0.5920 and accuracy 79.95%\t\tValid: loss 1.2310, accuracy 61.54%\n",
      "Epoch 86, CIFAR-10 Batch 1:  \tBatch: loss 0.6270 and accuracy 79.46%\t\tValid: loss 1.2145, accuracy 62.30%\n",
      "Epoch 86, CIFAR-10 Batch 2:  \tBatch: loss 0.6416 and accuracy 77.23%\t\tValid: loss 1.1720, accuracy 63.24%\n",
      "Epoch 86, CIFAR-10 Batch 3:  \tBatch: loss 0.5604 and accuracy 81.81%\t\tValid: loss 1.2016, accuracy 62.68%\n",
      "Epoch 86, CIFAR-10 Batch 4:  \tBatch: loss 0.5757 and accuracy 81.68%\t\tValid: loss 1.2104, accuracy 62.68%\n",
      "Epoch 86, CIFAR-10 Batch 5:  \tBatch: loss 0.5636 and accuracy 78.22%\t\tValid: loss 1.2225, accuracy 62.30%\n",
      "Epoch 87, CIFAR-10 Batch 1:  \tBatch: loss 0.6423 and accuracy 77.72%\t\tValid: loss 1.2346, accuracy 61.86%\n",
      "Epoch 87, CIFAR-10 Batch 2:  \tBatch: loss 0.5900 and accuracy 80.07%\t\tValid: loss 1.1909, accuracy 62.46%\n",
      "Epoch 87, CIFAR-10 Batch 3:  \tBatch: loss 0.5412 and accuracy 80.32%\t\tValid: loss 1.2182, accuracy 62.84%\n",
      "Epoch 87, CIFAR-10 Batch 4:  \tBatch: loss 0.5538 and accuracy 81.93%\t\tValid: loss 1.1802, accuracy 63.22%\n",
      "Epoch 87, CIFAR-10 Batch 5:  \tBatch: loss 0.5287 and accuracy 83.54%\t\tValid: loss 1.2058, accuracy 62.50%\n",
      "Epoch 88, CIFAR-10 Batch 1:  \tBatch: loss 0.5634 and accuracy 80.32%\t\tValid: loss 1.2232, accuracy 62.28%\n",
      "Epoch 88, CIFAR-10 Batch 2:  \tBatch: loss 0.5773 and accuracy 79.58%\t\tValid: loss 1.1752, accuracy 63.56%\n",
      "Epoch 88, CIFAR-10 Batch 3:  \tBatch: loss 0.5587 and accuracy 81.81%\t\tValid: loss 1.1997, accuracy 63.24%\n",
      "Epoch 88, CIFAR-10 Batch 4:  \tBatch: loss 0.5225 and accuracy 81.68%\t\tValid: loss 1.2067, accuracy 62.86%\n",
      "Epoch 88, CIFAR-10 Batch 5:  \tBatch: loss 0.5706 and accuracy 80.82%\t\tValid: loss 1.2098, accuracy 62.32%\n",
      "Epoch 89, CIFAR-10 Batch 1:  \tBatch: loss 0.5835 and accuracy 79.33%\t\tValid: loss 1.2160, accuracy 62.30%\n",
      "Epoch 89, CIFAR-10 Batch 2:  \tBatch: loss 0.5778 and accuracy 79.95%\t\tValid: loss 1.1944, accuracy 62.92%\n",
      "Epoch 89, CIFAR-10 Batch 3:  \tBatch: loss 0.5764 and accuracy 80.32%\t\tValid: loss 1.2275, accuracy 62.96%\n",
      "Epoch 89, CIFAR-10 Batch 4:  \tBatch: loss 0.6274 and accuracy 79.46%\t\tValid: loss 1.2485, accuracy 61.12%\n",
      "Epoch 89, CIFAR-10 Batch 5:  \tBatch: loss 0.5528 and accuracy 81.31%\t\tValid: loss 1.2279, accuracy 62.08%\n",
      "Epoch 90, CIFAR-10 Batch 1:  \tBatch: loss 0.5936 and accuracy 79.58%\t\tValid: loss 1.2658, accuracy 61.94%\n",
      "Epoch 90, CIFAR-10 Batch 2:  \tBatch: loss 0.5316 and accuracy 80.07%\t\tValid: loss 1.1844, accuracy 63.44%\n",
      "Epoch 90, CIFAR-10 Batch 3:  \tBatch: loss 0.5540 and accuracy 81.19%\t\tValid: loss 1.2237, accuracy 62.98%\n",
      "Epoch 90, CIFAR-10 Batch 4:  \tBatch: loss 0.5698 and accuracy 79.58%\t\tValid: loss 1.2337, accuracy 62.30%\n",
      "Epoch 90, CIFAR-10 Batch 5:  \tBatch: loss 0.5617 and accuracy 80.82%\t\tValid: loss 1.2119, accuracy 63.08%\n",
      "Epoch 91, CIFAR-10 Batch 1:  \tBatch: loss 0.5037 and accuracy 83.29%\t\tValid: loss 1.2573, accuracy 62.40%\n",
      "Epoch 91, CIFAR-10 Batch 2:  \tBatch: loss 0.5770 and accuracy 80.57%\t\tValid: loss 1.2134, accuracy 62.78%\n",
      "Epoch 91, CIFAR-10 Batch 3:  \tBatch: loss 0.5806 and accuracy 81.31%\t\tValid: loss 1.2556, accuracy 62.32%\n",
      "Epoch 91, CIFAR-10 Batch 4:  \tBatch: loss 0.5127 and accuracy 82.67%\t\tValid: loss 1.2298, accuracy 62.14%\n",
      "Epoch 91, CIFAR-10 Batch 5:  \tBatch: loss 0.4986 and accuracy 82.92%\t\tValid: loss 1.2135, accuracy 63.04%\n",
      "Epoch 92, CIFAR-10 Batch 1:  \tBatch: loss 0.5054 and accuracy 81.68%\t\tValid: loss 1.2315, accuracy 63.10%\n",
      "Epoch 92, CIFAR-10 Batch 2:  \tBatch: loss 0.5139 and accuracy 83.42%\t\tValid: loss 1.2014, accuracy 63.62%\n",
      "Epoch 92, CIFAR-10 Batch 3:  \tBatch: loss 0.5082 and accuracy 81.31%\t\tValid: loss 1.2197, accuracy 63.30%\n",
      "Epoch 92, CIFAR-10 Batch 4:  \tBatch: loss 0.5276 and accuracy 82.80%\t\tValid: loss 1.2326, accuracy 62.76%\n",
      "Epoch 92, CIFAR-10 Batch 5:  \tBatch: loss 0.5023 and accuracy 81.68%\t\tValid: loss 1.2157, accuracy 63.04%\n",
      "Epoch 93, CIFAR-10 Batch 1:  \tBatch: loss 0.4866 and accuracy 84.65%\t\tValid: loss 1.2452, accuracy 62.54%\n",
      "Epoch 93, CIFAR-10 Batch 2:  \tBatch: loss 0.5189 and accuracy 79.46%\t\tValid: loss 1.2131, accuracy 63.58%\n",
      "Epoch 93, CIFAR-10 Batch 3:  \tBatch: loss 0.4905 and accuracy 81.68%\t\tValid: loss 1.2585, accuracy 63.18%\n",
      "Epoch 93, CIFAR-10 Batch 4:  \tBatch: loss 0.5311 and accuracy 81.06%\t\tValid: loss 1.2549, accuracy 62.48%\n",
      "Epoch 93, CIFAR-10 Batch 5:  \tBatch: loss 0.5102 and accuracy 82.30%\t\tValid: loss 1.2278, accuracy 62.88%\n",
      "Epoch 94, CIFAR-10 Batch 1:  \tBatch: loss 0.4548 and accuracy 84.78%\t\tValid: loss 1.2349, accuracy 63.74%\n",
      "Epoch 94, CIFAR-10 Batch 2:  \tBatch: loss 0.5409 and accuracy 80.82%\t\tValid: loss 1.2035, accuracy 63.84%\n",
      "Epoch 94, CIFAR-10 Batch 3:  \tBatch: loss 0.5144 and accuracy 83.17%\t\tValid: loss 1.2100, accuracy 63.88%\n",
      "Epoch 94, CIFAR-10 Batch 4:  \tBatch: loss 0.4925 and accuracy 83.66%\t\tValid: loss 1.2251, accuracy 62.18%\n",
      "Epoch 94, CIFAR-10 Batch 5:  \tBatch: loss 0.4944 and accuracy 81.68%\t\tValid: loss 1.2531, accuracy 62.42%\n",
      "Epoch 95, CIFAR-10 Batch 1:  \tBatch: loss 0.5151 and accuracy 80.69%\t\tValid: loss 1.2698, accuracy 61.88%\n",
      "Epoch 95, CIFAR-10 Batch 2:  \tBatch: loss 0.5410 and accuracy 81.81%\t\tValid: loss 1.2191, accuracy 62.80%\n",
      "Epoch 95, CIFAR-10 Batch 3:  \tBatch: loss 0.4657 and accuracy 83.79%\t\tValid: loss 1.1987, accuracy 63.54%\n",
      "Epoch 95, CIFAR-10 Batch 4:  \tBatch: loss 0.4938 and accuracy 82.80%\t\tValid: loss 1.2404, accuracy 62.58%\n",
      "Epoch 95, CIFAR-10 Batch 5:  \tBatch: loss 0.4735 and accuracy 83.79%\t\tValid: loss 1.2238, accuracy 62.38%\n",
      "Epoch 96, CIFAR-10 Batch 1:  \tBatch: loss 0.5247 and accuracy 80.45%\t\tValid: loss 1.2494, accuracy 62.00%\n",
      "Epoch 96, CIFAR-10 Batch 2:  \tBatch: loss 0.5249 and accuracy 81.68%\t\tValid: loss 1.2246, accuracy 63.12%\n",
      "Epoch 96, CIFAR-10 Batch 3:  \tBatch: loss 0.5163 and accuracy 83.17%\t\tValid: loss 1.2490, accuracy 62.74%\n",
      "Epoch 96, CIFAR-10 Batch 4:  \tBatch: loss 0.4431 and accuracy 85.77%\t\tValid: loss 1.2321, accuracy 62.68%\n",
      "Epoch 96, CIFAR-10 Batch 5:  \tBatch: loss 0.4727 and accuracy 81.81%\t\tValid: loss 1.2218, accuracy 63.26%\n",
      "Epoch 97, CIFAR-10 Batch 1:  \tBatch: loss 0.5019 and accuracy 83.04%\t\tValid: loss 1.2479, accuracy 63.04%\n",
      "Epoch 97, CIFAR-10 Batch 2:  \tBatch: loss 0.5114 and accuracy 83.04%\t\tValid: loss 1.2314, accuracy 62.98%\n",
      "Epoch 97, CIFAR-10 Batch 3:  \tBatch: loss 0.4913 and accuracy 82.43%\t\tValid: loss 1.2227, accuracy 63.66%\n",
      "Epoch 97, CIFAR-10 Batch 4:  \tBatch: loss 0.4533 and accuracy 84.16%\t\tValid: loss 1.2370, accuracy 62.86%\n",
      "Epoch 97, CIFAR-10 Batch 5:  \tBatch: loss 0.4599 and accuracy 86.39%\t\tValid: loss 1.2249, accuracy 63.42%\n",
      "Epoch 98, CIFAR-10 Batch 1:  \tBatch: loss 0.4742 and accuracy 83.79%\t\tValid: loss 1.2682, accuracy 62.20%\n",
      "Epoch 98, CIFAR-10 Batch 2:  \tBatch: loss 0.4927 and accuracy 82.18%\t\tValid: loss 1.2446, accuracy 62.70%\n",
      "Epoch 98, CIFAR-10 Batch 3:  \tBatch: loss 0.4222 and accuracy 86.63%\t\tValid: loss 1.2421, accuracy 63.44%\n",
      "Epoch 98, CIFAR-10 Batch 4:  \tBatch: loss 0.4661 and accuracy 83.91%\t\tValid: loss 1.2327, accuracy 63.68%\n",
      "Epoch 98, CIFAR-10 Batch 5:  \tBatch: loss 0.4546 and accuracy 85.02%\t\tValid: loss 1.2495, accuracy 62.72%\n",
      "Epoch 99, CIFAR-10 Batch 1:  \tBatch: loss 0.4370 and accuracy 85.27%\t\tValid: loss 1.2778, accuracy 62.72%\n",
      "Epoch 99, CIFAR-10 Batch 2:  \tBatch: loss 0.5158 and accuracy 80.20%\t\tValid: loss 1.2510, accuracy 62.84%\n",
      "Epoch 99, CIFAR-10 Batch 3:  \tBatch: loss 0.4544 and accuracy 83.79%\t\tValid: loss 1.2377, accuracy 63.90%\n",
      "Epoch 99, CIFAR-10 Batch 4:  \tBatch: loss 0.4942 and accuracy 83.29%\t\tValid: loss 1.2433, accuracy 63.52%\n",
      "Epoch 99, CIFAR-10 Batch 5:  \tBatch: loss 0.4364 and accuracy 86.51%\t\tValid: loss 1.2543, accuracy 62.92%\n",
      "Epoch 100, CIFAR-10 Batch 1:  \tBatch: loss 0.4722 and accuracy 83.29%\t\tValid: loss 1.2674, accuracy 62.96%\n",
      "Epoch 100, CIFAR-10 Batch 2:  \tBatch: loss 0.5306 and accuracy 82.92%\t\tValid: loss 1.2650, accuracy 62.90%\n",
      "Epoch 100, CIFAR-10 Batch 3:  \tBatch: loss 0.4941 and accuracy 82.67%\t\tValid: loss 1.2745, accuracy 63.18%\n",
      "Epoch 100, CIFAR-10 Batch 4:  \tBatch: loss 0.4607 and accuracy 84.78%\t\tValid: loss 1.2689, accuracy 63.14%\n",
      "Epoch 100, CIFAR-10 Batch 5:  \tBatch: loss 0.4286 and accuracy 85.64%\t\tValid: loss 1.2755, accuracy 62.94%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6365393757820129\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nItH/A1mdJ8I6ma2vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222c46e67b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
